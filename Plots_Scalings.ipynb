{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baker\n"
     ]
    }
   ],
   "source": [
    "from mgkit.io import gff\n",
    "from mgkit import kegg\n",
    "import mgkit\n",
    "import mgkit.plots\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from mgkit.utils import dictionary\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from mgkit import graphs\n",
    "import os, sys\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import datetime\n",
    "import timeit\n",
    "import re\n",
    "import platform\n",
    "import getpass\n",
    "import argparse\n",
    "\n",
    "mgkit.logger.config_log()\n",
    "\n",
    "#On Server\n",
    "#input_dir=\"\"\n",
    "#output_dir=\"\"\n",
    "user=getpass.getuser()\n",
    "if \"windows\" in platform.platform().lower():\n",
    "    windows=True\n",
    "else:\n",
    "    windows=False\n",
    "if windows:\n",
    "    core=os.path.join(\"C:\\Users\",user)\n",
    "    g_drive=\"Google Drive\\Honours\"\n",
    "else:\n",
    "    core=os.path.join(\"/home\",user)\n",
    "    g_drive=\"grive/Honours\"\n",
    "print core\n",
    "\n",
    "#Made some OS agnostic changes\n",
    "#gff_dir=os.path.join(*[core,\"Documents\",\"Hons\",\"Seaquence\",\"francesco_data\",\"gff_bins-2016-06-14\"])\n",
    "gff_dir=os.path.join(*[core,\"Documents\",\"Hons\",\"Seaquence\",\"francesco_data\",\"gff_bins-2016-06-14b\"])\n",
    "#gff_dir=core+\"/Documents/Hons/Seaquence/francesco_data/gff_bins-2016-06-14\"\n",
    "\n",
    "tax_file=os.path.join(*[core,g_drive,\"metabolic_analysis\",\"ID_TAX_BINS_TEMP.txt\"])\n",
    "shortened_tax_file=os.path.join(*[core,g_drive,\"metabolic_analysis\",\"Non_uniq_shortened_ID.txt\"])\n",
    "#tax_file=core+\"/grive/metabolic_analysis/ID_TAX_BINS_TEMP.txt\"\n",
    "\n",
    "output_dir=os.path.join(*[core,g_drive,\"metabolic_analysis\"])\n",
    "#output_dir=core+\"/grive/metabolic_analysis/\"\n",
    "\n",
    "coral_kegg=os.path.join(*[output_dir,\"KO_hits\",\"plut.pathways.txt\"])\n",
    "#coral_kegg=output_dir+\"KO_hits/plut.pathways.txt\"\n",
    "\n",
    "cmn_cpds=os.path.join(*[output_dir,\"Automated_Network_Analyses\",\"boring_cps.txt\"])\n",
    "#cmn_cpds=output_dir+\"Automated_Network_Analyses/boring_cps.txt\"\n",
    "\n",
    "\n",
    "symbiodinium_kegg=os.path.join(*[output_dir,\"KO_hits\",\"SymbC15_firstpass_ko_mapping_protID_distinct_KO.txt\"])\n",
    "#symbiodinium_kegg=output_dir+\"KO_hits/SymbC15_firstpass_ko_mapping_protID_distinct_KO.txt\"\n",
    "\n",
    "#microbial_kegg=os.path.join(*[output_dir,\"KO_hits\",\"Microbial_KO_mapping_protID.txt\"])\n",
    "microbial_kegg=os.path.join(*[output_dir,\"KO_hits\",\"TREMBL_SWISSPROT_Microbial_KO_mapping_protID.txt\"])\n",
    "#microbial_kegg=output_dir+\"KO_hits/Microbial_KO_mapping_protID.txt\"\n",
    "all_kegg=os.path.join(*[output_dir,\"KO_hits\",\"all_kos.txt\"])\n",
    "\n",
    "hmm_dir=os.path.join(*[core,g_drive,\"HMM_searches\",\"Symbioses_test\",\"euk_repeat_results\"])\n",
    "\n",
    "database_dir=os.path.join(output_dir,\"Databases\")\n",
    "\n",
    "abundance_file=os.path.join(*[output_dir,\"Misc_files\",\"id_trimmed_relative_enriched_bin_abundance.tsv\"])\n",
    "\n",
    "raw_coverage_contig=os.path.join(*[output_dir,\"Misc_files\",\"bin_contig_sep_coverages.tsv\"])\n",
    "\n",
    "completeness_contamination=os.path.join(*[output_dir,\"Misc_files\",\"Completeness_Contamination_data.txt\"])\n",
    "\n",
    "gene_dir=os.path.join(*[core,g_drive,\"eukaryote_like_repeats\",\"gene_hits\"])#\"/home/baker/Documents/MountedDrive/seaquence/data/eukaryote_like_repeats/gene_hits\"\n",
    "\n",
    "plots_dir=os.path.join(output_dir,'Plots')\n",
    "\n",
    "uniq_tax_file=os.path.join(*[output_dir, \"BIN_uniq_shortened_ID.txt\"])\n",
    "\n",
    "def load_cmn_cpds(cmn_cpds):\n",
    "    cpds=set([])\n",
    "    with open(cmn_cpds,'r') as cpd_list:\n",
    "        for line in cpd_list:\n",
    "            cpd=line.strip().split(\"\\t\")[0]\n",
    "            cpds.add(cpd)\n",
    "    return cpds\n",
    "            \n",
    "common_cpds=load_cmn_cpds(cmn_cpds)\n",
    "\n",
    "# General Data loading\n",
    "\n",
    "def store_local_kegg_item_keys(kegg_items,database_dir):\n",
    "    kc=kegg.KeggClientRest()\n",
    "    all_item_names={}\n",
    "    illegal_pairs=[(\"compound\",\"orthology\"),(\"orthology\",\"compound\")]\n",
    "    for kegg_item in kegg_items:\n",
    "        key_name=os.path.join(database_dir,\"{0}_readable_names.tsv\".format(kegg_item))\n",
    "        if not os.path.isfile(key_name):\n",
    "            item_names=kc.get_ids_names(kegg_item)\n",
    "            save_readable_key(key_name,item_names,kegg_item)\n",
    "            all_item_names[kegg_item]=item_names.keys()\n",
    "        else:\n",
    "            all_item_names[kegg_item]=load_readable_names(database_dir,[kegg_item],False)[kegg_item].keys()\n",
    "    for kegg_item_1, kegg_item_2 in itertools.permutations(all_item_names.iterkeys(),2):\n",
    "        print \"considering the pair: {0}, {1}\".format(kegg_item_1,kegg_item_2)\n",
    "        if (kegg_item_1,kegg_item_2) not in illegal_pairs:\n",
    "            shared_key_name=os.path.join(database_dir,\"{0}_linked_{1}_database.tsv\").format(kegg_item_1, kegg_item_2)\n",
    "            if not os.path.isfile(shared_key_name):\n",
    "                print \"The processing of pair: {0},{1} has begun.\".format(kegg_item_1,kegg_item_2)\n",
    "                kc=kegg.KeggClientRest()\n",
    "                linked_ids=kc.link_ids(kegg_item_2,all_item_names[kegg_item_1])\n",
    "                save_key_pairings(shared_key_name,linked_ids,(kegg_item_1,kegg_item_2))\n",
    "        else:\n",
    "            pass\n",
    "    return\n",
    "\n",
    "def load_local_kegg_database_pairings(database_dir,kegg_item_pairs, process_all):\n",
    "    '''Loads the local databases of kegg_item_1, kegg_item_2 pairings and return a dictionary of\n",
    "    these pairings in the form kegg_item_1:kegg_items_2 (There can be more than one linked item). This\n",
    "    loading is based on the earlier use of mgkits kc.link_ids to store all of the pairings needed.\n",
    "    \n",
    "    Input:\n",
    "        database_dir   - The directory with the databases\n",
    "        kegg_item_pairs- A list of kegg item pairs to load\n",
    "        process_all    - A boolean decision as whether to load all existing pairs.\n",
    "        \n",
    "    Output: A dictionary linking either all existing kegg item pairs or just those specified. It has the form\n",
    "    dict[item_1,item_2]={kegg_item_1:kegg_2_items}'''\n",
    "    linking_dictionary={}\n",
    "    if process_all:\n",
    "        for file_name in glob(os.path.join(database_dir,'*database.tsv')):\n",
    "            db_file=os.basename(file_name)\n",
    "            kegg_1=db_file.split(\"_linked_\")[0]\n",
    "            kegg_2=db_file.split(\"_linked_\")[1].split(\"_database\")[0]\n",
    "            linking_dictionary[(kegg_1,kegg_2)]={}             \n",
    "            with open(file_name) as kegg_links:\n",
    "                next(kegg_links)#Skip the header\n",
    "                for line in kegg_links:\n",
    "                    item_1,item_2=line.strip().split(\"\\t\")\n",
    "                    item_2=item_2.split(\";\")\n",
    "                    linking_dictionary[kegg_item_pair][item_1]=item_2\n",
    "        return linking_dictionary\n",
    "    \n",
    "    for kegg_item_pair in kegg_item_pairs:\n",
    "        file_name=os.path.join(database_dir,\"{0}_linked_{1}_database.tsv\").format(kegg_item_pair[0], kegg_item_pair[1])\n",
    "        if os.path.isfile(file_name):\n",
    "            linking_dictionary[kegg_item_pair]={}\n",
    "            with open(file_name) as kegg_links:\n",
    "                next(kegg_links) #skip the header\n",
    "                for line in kegg_links:\n",
    "                    item_1,item_2=line.strip().split(\"\\t\")\n",
    "                    item_2=item_2.split(\";\")\n",
    "                    linking_dictionary[kegg_item_pair][item_1]=item_2\n",
    "    return linking_dictionary\n",
    "                    \n",
    "def load_readable_names(database_dir,kegg_items,process_all):\n",
    "    '''Loads in the readable names for a specified kegg item from a list of databases.\n",
    "    Input:\n",
    "        database_dir        -  The directory with the databases.\n",
    "        kegg_items          -  The kegg items to get the readable mapping for.\n",
    "        process_all         -  Boolean - Should the function retrieve all available databases.\n",
    "    Output:\n",
    "        readable_item_dict  -  A dictionary of KEGG_ID: Readable name pairs'''\n",
    "    readable_item_dict={}\n",
    "    if process_all:\n",
    "        for file_name in glob(os.path.join(database_dir,'*_readable_names.tsv')):\n",
    "            desc_file=os.basename(file_name)\n",
    "            kegg_item=desv_file.split(\"_readable_names.tsv\")[0]\n",
    "            readable_item_dict[kegg_item]={}\n",
    "            with open(file_name) as kegg_descriptions:\n",
    "                next(kegg_descriptions)\n",
    "                for line in kegg_descriptions:\n",
    "                    item_1,item_2=line.strip().split(\"\\t\")\n",
    "                    readable_item_dict[kegg_item][item_1]=item_2\n",
    "        return readable_item_dict\n",
    "    \n",
    "    for kegg_item in kegg_items:\n",
    "        file_name=os.path.join(database_dir,'{0}_readable_names.tsv'.format(kegg_item))\n",
    "        readable_item_dict[kegg_item]={}\n",
    "        with open(file_name) as kegg_descriptions:\n",
    "            next(kegg_descriptions)\n",
    "            for line in kegg_descriptions:\n",
    "                item_1,item_2=line.strip().split(\"\\t\")\n",
    "                readable_item_dict[kegg_item][item_1]=item_2\n",
    "    return readable_item_dict\n",
    "            \n",
    "            \n",
    "def save_readable_key(key_name,item_names,kegg_item):\n",
    "    df=pd.DataFrame([\n",
    "    [col1,col2] for col1,col2 in item_names.iteritems()\n",
    "                   ])\n",
    "    df.columns=[kegg_item,\"Description\"]\n",
    "    df.to_csv(key_name,sep=\"\\t\",index=None)\n",
    "    return None\n",
    "\n",
    "def save_key_pairings(shared_key_name,item_links,kegg_item_tuple):\n",
    "    df=pd.DataFrame([\n",
    "            [col1,\";\".join(col2)] for col1, col2 in item_links.iteritems()\n",
    "        ])\n",
    "    df.columns=[kegg_item_tuple[0],kegg_item_tuple[1]]\n",
    "    df.to_csv(shared_key_name,sep=\"\\t\",index=None)\n",
    "    return None\n",
    "\n",
    "def remove_ko_pth_hits(file_path):\n",
    "    \n",
    "    with open(file_path,'r') as KO_PTH_pairs:\n",
    "        out_dir=os.path.dirname(file_path)\n",
    "        temp_file=open(os.path.join(out_dir,\"temp.tsv\"),'w')\n",
    "        for line in KO_PTH_pairs:\n",
    "            KO,pathways=line.strip().split(\"\\t\")\n",
    "            pathways=pathways.split(\";\")\n",
    "            pathways=[pathway for pathway in pathways if not pathway.startswith(\"ko\")]\n",
    "            pathways=\";\".join(pathways)\n",
    "            new_line=\"{0}\\t{1}\\n\".format(KO,pathways)\n",
    "            temp_file.write(new_line)\n",
    "    temp_file.close()\n",
    "    \n",
    "def make_local_rcn_eqn_database(database_dir):\n",
    "    kc=kegg.KeggClientRest()\n",
    "    all_reactions=load_readable_names(database_dir,[\"reaction\"],False)[\"reaction\"].keys()\n",
    "    rcn_eqns=kc.get_reaction_equations(all_reactions,max_len=10)\n",
    "    file_name=os.path.join(database_dir, \"reaction_equation_links.tsv\")\n",
    "    df=rcn_eqn_pd_df(rcn_eqns)\n",
    "    df.to_csv(file_name,sep=\"\\t\",index=False)\n",
    "        \n",
    "    return\n",
    "\n",
    "def rcn_eqn_pd_df(rcn_eqn_dict):\n",
    "    df=pd.DataFrame([\n",
    "            [rcn, \";\".join(in_cpds),\";\".join(out_cpds)] for rcn, cpds in rcn_eqn_dict.iteritems() for in_cpds,out_cpds in [cpds.values()]\n",
    "    if in_cpds!=[] or out_cpds!=[]    \n",
    "        ])\n",
    "    #df.replace('','NA')\n",
    "    df.columns=[\"Kegg_rcn_ID\",\"side_1_cpds\",\"side_2_cpds\"]\n",
    "    return df\n",
    "\n",
    "def load_local_rcn_eqn_database(database_dir):\n",
    "    file_name=os.path.join(database_dir,\"reaction_equation_links.tsv\")\n",
    "    rcn_eqn_dict={}\n",
    "    n_df=pd.read_csv(file_name,sep=\"\\t\")\n",
    "    n_df.fillna('',inplace=True)\n",
    "    return n_df.set_index(\"Kegg_rcn_ID\").T.to_dict(orient='dict')\n",
    "\n",
    "def load_local_rcn_eqn_database_set(database_dir):\n",
    "    rcn_eqn_pairs=load_local_rcn_eqn_database(database_dir)\n",
    "    return {rcn:{side:set(cpds.split(\";\")) for side,cpds in pairs.iteritems()} for rcn, pairs in rcn_eqn_pairs.iteritems()}\n",
    "            \n",
    "#Load in the coral data\n",
    "def load_bin_names(tax_file):\n",
    "    #Load bin_ids and bins_taxonomy from file.\n",
    "    bin_names={}\n",
    "    bin_pair=[]\n",
    "    with open(tax_file,'r') as bin_tax_pair:\n",
    "        bin_tax_pair.readline()\n",
    "        for line in bin_tax_pair:\n",
    "            bin_pair.append(tuple(line.strip().split(\"\\t\")))\n",
    "\n",
    "    bin_names={bin_id:taxonomy for taxonomy, bin_id in bin_pair}\n",
    "    return bin_names\n",
    "\n",
    "def make_local_complete_module_info_db(database_dir):\n",
    "    '''Creates a local database of the module definitions.'''\n",
    "    all_modules=load_readable_names(database_dir,[\"module\"],False)[\"module\"].keys()\n",
    "    kc=kegg.KeggClientRest()\n",
    "    entries={}\n",
    "    max_len=10\n",
    "    post_processed_defs={}\n",
    "    print \"There are a total of {0} modules to parse\".format(len(all_modules))\n",
    "    N_modules=len(all_modules)\n",
    "    for i in xrange(0,N_modules,max_len):\n",
    "        if N_modules-i<max_len:\n",
    "            n_entries=N_modules-i\n",
    "        else:\n",
    "            n_entries=max_len      \n",
    "        query=\"+\".join(all_modules[i:i+n_entries])\n",
    "        kegg_entries=kc.get_entry(query)\n",
    "        hits=re.findall(\"\\nDEFINITION(.*)\\n\",kegg_entries)\n",
    "        print i, i+max_len-1,\"n_hits:{0}\".format(len(hits))\n",
    "        #if len(hits)!=max_len:\n",
    "        #    print m\n",
    "        for module, definition in itertools.izip(all_modules[i:i+10],hits):\n",
    "            new_def=definition.strip().replace(\" --\",\" \").replace(\"-- \",\" \").replace(\"  \",\" \").strip()\n",
    "            entries[module]=new_def\n",
    "            if \"M\" in definition:\n",
    "                print module,definition\n",
    "                post_processed_defs[module]=new_def\n",
    "        #These post_processed modules should be modules defined in terms of other modules.\n",
    "    for module, definition in post_processed_defs.iteritems():\n",
    "        new_def=definition\n",
    "        print \"This is the definition being considered.\", new_def\n",
    "        new_defs=re.split(\"[, +-]\",definition)\n",
    "        for item in new_defs:\n",
    "            simp_item=item.strip(\")\").strip(\"(\")\n",
    "            if simp_item.startswith(\"M\"):\n",
    "                print \"This is the current item\",item\n",
    "                new_def=new_def.replace(simp_item,\"(\"+entries[simp_item]+\")\")\n",
    "        print new_def\n",
    "        entries[module]=new_def\n",
    "        \n",
    "    temp_entries=entries\n",
    "    protein_complexes='(.)?([K][0-9]+[+]){1,}[K][0-9]+(.)?'\n",
    "    for module, definition in temp_entries.iteritems():\n",
    "        for match in re.finditer(protein_complexes, definition):\n",
    "            match_str=match.group()\n",
    "            if match_str.startswith(\"(\") and match_str.endswith(\")\"):\n",
    "                pass\n",
    "            elif match_str[-1].isdigit() and match_str[0]==\"K\":\n",
    "                match_str=match_str[:] #Trim random end characters\n",
    "                new_match=\"(\"+match_str+\")\"\n",
    "                entries[module]=entries[module].replace(match_str,new_match)\n",
    "            elif match_str[0]==\"K\":\n",
    "                match_str=match_str[0:-1] #Trim random end characters\n",
    "                new_match=\"(\"+match_str+\")\"\n",
    "                entries[module]=entries[module].replace(match_str,new_match)\n",
    "            elif match_str[-1].isdigit():\n",
    "                match_str=match_str[1:] #Trim random end characters\n",
    "                new_match=\"(\"+match_str+\")\"\n",
    "                entries[module]=entries[module].replace(match_str,new_match)\n",
    "            else:\n",
    "                match_str=match_str[1:-1] #Trim random end characters\n",
    "                new_match=\"(\"+match_str+\")\"\n",
    "                entries[module]=entries[module].replace(match_str,new_match)\n",
    "              \n",
    "        if i%100==0:\n",
    "            kc=kegg.KeggClientRest()\n",
    "            print module\n",
    "    print \"{0} modules were parsed\".format(len(entries))\n",
    "    df=pd.DataFrame([\n",
    "            [module,entry] for module,entry in entries.iteritems()\n",
    "        ])\n",
    "    df.columns=[\"Kegg_id\",\"Kegg_definition\"]\n",
    "    df.to_csv(os.path.join(database_dir,\"Module_definitions_pairs_db.tsv\"),sep=\"\\t\",index=None)\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_local_complete_module_info_db(database_dir):\n",
    "    '''Loads a local database of kegg definitions'''\n",
    "    def_dict={}\n",
    "    with open(os.path.join(database_dir,\"Module_definitions_pairs_db.tsv\")) as definitions:\n",
    "        next(definitions)\n",
    "        for line in definitions:\n",
    "            module,kegg_def=line.strip().split(\"\\t\")\n",
    "            def_dict[module]=kegg_def\n",
    "    \n",
    "    return def_dict\n",
    "\n",
    "def make_new_trusted_database(database_dir):\n",
    "    '''\n",
    "    Definition:\n",
    "        This function will take an entire kegg module definition file and will create\n",
    "        a new local database with the expressions written so that they can simple be\n",
    "        evaluated when loading the files.\n",
    "    Input: \n",
    "        database_dir: str\n",
    "            A directory containing the database of kegg module definitions.\n",
    "    Output:\n",
    "        None\n",
    "    Calls:\n",
    "        replacement: Turns kegg definitions in logical nested tuples of sets.\n",
    "    '''\n",
    "    old_module_def=load_local_complete_module_info_db(database_dir)\n",
    "    #print \"This is the old module information\",old_module_def\n",
    "    new_pd_df=[\"\"]*len(old_module_def)\n",
    "    i=0\n",
    "    for module,definition in old_module_def.iteritems():\n",
    "        try:\n",
    "            logical_evaluation=replacement(definition,False)[1]\n",
    "            new_pd_df[i]=[module,logical_evaluation]\n",
    "            i+=1\n",
    "                \n",
    "        except TypeError:\n",
    "            print \"TypeError 2:\",module, definition\n",
    "        except SyntaxError:\n",
    "            print \"SyntaxError 2:\",module, definition\n",
    "        except NameError:\n",
    "            print \"NameError 2:\",module ,definition\n",
    "    #print new_pd_df      \n",
    "    new_pd_df=pd.DataFrame(new_pd_df)\n",
    "    #print \"The second checkpoint.\"\n",
    "    new_pd_df.columns=[\"ModuleID\",\"KEGG_log_expr\"]\n",
    "    \n",
    "    new_pd_df.to_csv(os.path.join(database_dir, \"module_kegg_log_expr.tsv\"),header=True,sep=\"\\t\",index=False)\n",
    "    return None\n",
    "\n",
    "def fix_module_orthology_pairs(database_dir):\n",
    "    '''Replaces the occurences of modules in the module-orthology links to their corresponding KOs'''\n",
    "    all_pairs=load_local_kegg_database_pairings(database_dir,[[\"module\",\"orthology\"]], False)[\"module\",\"orthology\"]\n",
    "    new_pairings=[]\n",
    "    for module,kos in all_pairs.iteritems():\n",
    "        new_items=[]\n",
    "        rand_module=False\n",
    "        for item in kos:\n",
    "            if item.lower().startswith(\"m\"):\n",
    "                new_items.extend(all_pairs[item])\n",
    "                rand_module=True\n",
    "            else:\n",
    "                new_items.append(item)\n",
    "        if rand_module:\n",
    "            new_pairings.append((module, list(set(new_items))))\n",
    "    for (module, new_items) in new_pairings:\n",
    "        all_pairs[module]=new_items\n",
    "        \n",
    "    df=pd.DataFrame([\n",
    "            [col1,\";\".join(col2)] for col1, col2 in all_pairs.iteritems()\n",
    "        ])\n",
    "    df.columns=[kegg_item_tuple[0],kegg_item_tuple[1]]\n",
    "    fixed_name=os.path.join(database_dir,\"\")\n",
    "    df.to_csv(fixed_name,sep=\"\\t\",index=None)\n",
    "    return\n",
    "        \n",
    "def load_local_cleaned_definition_db(database_dir):\n",
    "    cleaned_db={}\n",
    "    with open(os.path.join(database_dir,\"module_kegg_log_expr.tsv\")) as paired_exprs:\n",
    "        next(paired_exprs) #Skip header\n",
    "        for line in paired_exprs:\n",
    "            module,expr=line.split(\"\\t\")\n",
    "            kegg_log=eval(expr)\n",
    "            if not isinstance(kegg_log,tuple):\n",
    "                kegg_log=tuple([kegg_log])\n",
    "            cleaned_db[module]=kegg_log\n",
    "            \n",
    "    return cleaned_db\n",
    "\n",
    "# For remaking graphs\n",
    "\n",
    "pathways = {\n",
    "    'carbon': ['map01200'],\n",
    "    'nitrogen-sulfur-fatty_acid-photosynthesis': ['map00910', 'map00920', 'map01212', 'map00195'],\n",
    "    'oxidative_phosphorylation': ['map00190'],\n",
    "    'two-component': ['map02020'],\n",
    "    'amino-acids':['map01230'],\n",
    "    #'thiamine-metabolism':'map00730',\n",
    "    #'riboflavin-metabolism':'map00740',\n",
    "    #'Vitamin-B6-metabolism':'map00750',\n",
    "    #'Nicotinate&Nicotinamide-metabolism':'map00760',\n",
    "    #'PantoThenate and CoA Biosynthesis':'map00770',\n",
    "    #'Biotin-Metabolism':'map00780',\n",
    "    #'Lipoic-Acid-Metabolism':'map00785',\n",
    "    #'Folate-Biosynthesis':'map00790',\n",
    "    #'OneCarbonPoolByFolate':'map00670',\n",
    "    #'retinol-metabolism-animals':'map00830',\n",
    "    #'porphyrin&ChlorophyllMetabolism':'map00860',\n",
    "    #'Ubiquinone&OtherTerpenoid-QuinoneBiosynthesis':'map00130',\n",
    "    'vitamins&cofactors':['map00730','map00740','map00750','map00760','map00770','map00780','map00785','map00790','map00670','map00830','map00860','map00130'],\n",
    "    #\"Alanine, aspartate and glutamate metabolism\":'map00250',\n",
    "    #\"Cysteine and methionine metabolism\":'map00270',\n",
    "    #\"Glycine, serine and threonine metabolism\":'map00260',\n",
    "    #\"Valine, leucine and isoleucine degradation\":'map00280',\n",
    "    #\"Valine, leucine and isoleucine biosynthesis\":'map00290',\n",
    "    #\"Lysine biosynthesis\":'map00300',\n",
    "    #\"Lysine degradation\":'map00310',\n",
    "    #\"Arginine biosynthesis\":'map00220',\n",
    "    #\"Arginine and proline metabolism\":'map00330',\n",
    "    #\"Histidine metabolism\":'map00340',\n",
    "    #\"Tyrosine metabolism\":'map00350',\n",
    "    #\"Phenylalanine metabolism\":'map00360',\n",
    "    #\"Tryptophan metabolism\":''map00250'map00380',\n",
    "    #\"Phenylalanine, tyrosine and tryptophan biosynthesis\":'map00400',\n",
    "    \"AminoAcidMetabolism\":['map00270','map00260','map00280','map00290','map00300','map00310','map00220',\\\n",
    "                           'map00330','map00340','map00350','map00360','map00380','map00400']\n",
    "    #\"beta-Alanine metabolism\":'map00410',\n",
    "    #\"Taurine and hypotaurine metabolism\":'map00430',\n",
    "    #\"Phosphonate and phosphinate metabolism\":'map00440',\n",
    "    #\"Selenocompound metabolism\":'map00450',\n",
    "    #\"Cyanoamino acid metabolism\":'map00460',\n",
    "    #\"D-Glutamine and D-glutamate metabolism\":'map00471',\n",
    "    #\"D-Arginine and D-ornithine metabolism\":'map00472',\n",
    "    #\"D-Alanine metabolism\":'map00473',\n",
    "    #\"Glutathione metabolism\":'map00480',\n",
    "   # \"Metabolisms of other amino acids\":['map00410','map00430','map00440','map00450','map00460','map00471','map00472','map00473','map00480']\n",
    "    ,\"Glycosaminoglycan degradation & Synthesis\":[\"map00531\",\"map00532\",\"map00534\"] ,\n",
    "    \"Bacterial Secretion Systems\":[\"ko03070\"],\n",
    "    \"phosphotransferase system (PTS)\":[\"ko02060\"],\n",
    "    \"ABC transporters\":[\"ko02010\"],\n",
    "    \"N-Glycan biosynthesis\": [\"map00510\"],\n",
    "    \"CationicAntiomicrobialPeptide_CAMP_resistance\":[\"map01503\"],\n",
    "    \"Vancomycin_Beta-lactamResistance\":[\"map01502\",\"map01501\"],\n",
    "    \"Sulfatases\":[]\n",
    "\n",
    "}\n",
    "\n",
    "pathways ={key:[item.replace(\"ko\",\"map\") for item in items] for key,items in pathways.iteritems()}\n",
    "\n",
    "bin_names=load_bin_names(tax_file)\n",
    "\n",
    "def pathway_to_modules(pathway_dict,database_dir):\n",
    "    links=load_local_kegg_database_pairings(database_dir,[(\"pathway\",\"module\")], False)[\"pathway\",\"module\"]\n",
    "    pathways={path:list(set(itertools.chain(*[links[egx] for egx in pathway if egx in links]))) for path, pathway in pathway_dict.iteritems()}\n",
    "    return pathways\n",
    "\n",
    "MO_pathways=pathway_to_modules(pathways,database_dir)\n",
    "\n",
    "####################################\n",
    "# Abundance parsing\n",
    "####################################\n",
    "def load_relative_abundance(file_name):\n",
    "    abundance_data=pd.DataFrame.from_csv(file_name,sep=\"\\t\")\n",
    "    new_index=np.array(pd.Series(abundance_data.index.values).str.strip(\"_genomic\"))\n",
    "    abundance_data.set_index(new_index,inplace=True)\n",
    "    return abundance_data\n",
    "\n",
    "\n",
    "def rel_abundance_to_dict(df):\n",
    "    abundance_dict={}\n",
    "    column_names=df.columns.values\n",
    "    for genome, rel_abund in df.iterrows():\n",
    "        abundance_dict[genome]={}\n",
    "        for i,column_name in enumerate(column_names):\n",
    "            abundance_dict[genome][column_name]=round(rel_abund[i],9)\n",
    "    return abundance_dict\n",
    "\n",
    "def get_abundance(file_name):\n",
    "    return rel_abundance_to_dict(load_relative_abundance(file_name))\n",
    "\n",
    "def only_key_abundance(abundance_dict,unique_key):\n",
    "    reduced_dict={}\n",
    "    for genome, rel_abund_dict in abundance_dict.iteritems():\n",
    "        for sample, abundance in rel_abund_dict.iteritems():\n",
    "            if unique_key in sample:\n",
    "                reduced_dict[genome]=abundance\n",
    "    return reduced_dict\n",
    "\n",
    "def reduced_abundance(file_name,unique_key):\n",
    "    \n",
    "    return only_key_abundance(get_abundance(file_name),unique_key)\n",
    "    \n",
    "\n",
    "def load_coverages(file_name):\n",
    "    '''Loads in the coverage file with a separate coverage for each contig. '''\n",
    "    with open(file_name) as coverage_file:\n",
    "        header=coverage.readline()\n",
    "    header=tuple(header.strip().split(\"\\t\"))\n",
    "    coverage_file=np.genfromtxt(fule_name,delimiter=\"\\t\",names=True)\n",
    "    return\n",
    "\n",
    "def normalised_coverages(read_counts=True):\n",
    "    return\n",
    "\n",
    "def load_completeness_contamination(file_name):\n",
    "    genome_data=pd.DataFrame.from_csv(file_name,sep=\"\\t\")\n",
    "    genome_data_dict={}\n",
    "    data_names=[\"completeness\",\"contamination\"]\n",
    "    for genome, row in genome_data.iterrows():\n",
    "        genome_data_dict[genome]={}\n",
    "        for item_id,item in itertools.izip(data_names,row):\n",
    "            genome_data_dict[item_id]=item\n",
    "    return genome_data\n",
    "\n",
    "#def \n",
    "\n",
    "def estimate_binning_completeness(coverage_file,completeness_file, total_counts_file):\n",
    "    \n",
    "    return\n",
    "\n",
    "#################################\n",
    "# KEGG Completeness\n",
    "#################################\n",
    "\n",
    "def replacement(definition,return_string=False):\n",
    "    '''\n",
    "    Description:\n",
    "        Turns an irregular definition string into a set of KOs in nested tuples to indicate their relationship and to\n",
    "        prepare them for processing.\n",
    "    Input:\n",
    "        definition: string\n",
    "            Module definition as defined in KEGG    \n",
    "    output:\n",
    "        definition: Same as above\n",
    "        new_nesting: tuple of tuples of sets\n",
    "            The new logical form of the definition to use in evaluating completeness.\n",
    "        \n",
    "    Notes: \n",
    "        This function uses eval which a security risk. Caution should be taken in using this function\n",
    "    '''\n",
    "    \n",
    "    logical_chars=\"[+ ,-]\"\n",
    "    pattern=\"K[0-9]{5}\"\n",
    "    new_expression=definition\n",
    "    logical_groups=\"([K][0-9]+,){1,}[K][0-9]+\" #Find any group of KOs (1 or more) separated by commas\n",
    "    #non_extended_groups='(K[0-9]{5}[^,K0-9\\n\\]]*){1,}(K[0-9]{5}[^,])' #Get any none comma separated chunk of KOs\n",
    "    non_extended_groups='([^,]|^)(K[0-9]{5}[^,K0-9\\n\\]\\)\\[]*){1,}([\\n \\+\\\"\\]\\[\\(\\)]|$)'\n",
    "    end_KOs='(K[0-9]{5})$'\n",
    "    repeated_set='(set\\(\\[){2,}([\"][K][0-9]+[\"][^^ )(+.\\\"-]?).*?(\\]\\)){1,2}'\n",
    "    set_in_set='set\\(\\[[K0-9\",]*(set\\(\\[)\"[K][0-9]+\"\\]\\)'\n",
    "    rear_match=\"([^0-9][,-]K[0-9]{5})\"\n",
    "    forward_match='(K[0-9]{5})[,-][\\(]'\n",
    "    new_expression=new_expression.replace(\" -\",\"-\")\n",
    "    new_expression=new_expression.replace(\", \",\",\")\n",
    "    ko_set_matches=[]\n",
    "        \n",
    "    for match in re.finditer(logical_groups,new_expression):\n",
    "        #print match.group()\n",
    "        ko_set_matches.append(match.group())\n",
    "        #new_expression=new_expression.replace(match.group(),\"set([\"+match.group()+\"])\",1)\n",
    "    set_matches=['']*len(ko_set_matches)\n",
    "    cleaned_matches=[match.strip(\",\") for match in ko_set_matches]\n",
    "    ko_set_matches=set(cleaned_matches)\n",
    "    \n",
    "    \n",
    "    for i,match in enumerate(ko_set_matches):\n",
    "        set_matches[i]=match\n",
    "        new_expression=new_expression.replace(match,\"set([\"+match+\"])\")\n",
    "        \n",
    "    step_0_1=new_expression\n",
    "    for match in re.finditer(non_extended_groups,new_expression):\n",
    "        if match:\n",
    "            new_match=match.group()\n",
    "            #print type(new_match), new_match\n",
    "            for sub_match in re.findall(pattern,new_match):\n",
    "                new_match=new_match.replace(sub_match,\"set([\"+sub_match+\"])\")\n",
    "            new_expression=new_expression.replace(match.group(),new_match)\n",
    "            \n",
    "    for match in re.finditer(rear_match,new_expression):\n",
    "        if match:\n",
    "            new_match=match.group()\n",
    "            #print type(new_match), new_match\n",
    "            for sub_match in re.findall(pattern,new_match):\n",
    "                new_match=new_match.replace(sub_match,\"set([\"+sub_match+\"])\")\n",
    "            new_expression=new_expression.replace(match.group(),new_match)\n",
    "            \n",
    "    for match in re.finditer(forward_match,new_expression):\n",
    "        if match:\n",
    "            new_match=match.group()\n",
    "            #print type(new_match), new_match\n",
    "            for sub_match in re.findall(pattern,new_match):\n",
    "                new_match=new_match.replace(sub_match,\"set([\"+sub_match+\"])\")\n",
    "            #print new_match\n",
    "            new_expression=new_expression.replace(match.group(),new_match)\n",
    "                \n",
    "    step_0_2=new_expression\n",
    "    for match in set(re.findall(pattern,new_expression)):\n",
    "        new_expression=new_expression.replace(match,\"\\\"\"+match+\"\\\"\")\n",
    "    step_1=new_expression\n",
    "    #print new_expression\n",
    "    \n",
    "    new_expression=new_expression.replace(\",\",\",\\\",\\\",\")\n",
    "    #non_set_comma='.{6}[^\\\"],[^\\\"].{6}' #Extends sides to try and ensure uniqueness\n",
    "    #for match in set(re.findall(non_set_comma,new_expression)):\n",
    "    #    new_expression=new_expression.replace(match,\",\\\",\\\",\".join(match.split(\",\")))\n",
    "        \n",
    "    new_expression=new_expression.replace(\" \",\",\\\" \\\",\")\n",
    "    #print new_expression\n",
    "    new_expression=new_expression.replace(\"-\",\",\\\"-\\\",\")\n",
    "    new_expression=new_expression.replace(\"+\",\",\\\"+\\\",\")\n",
    "    step_2=new_expression\n",
    "    #print new_expression\n",
    "    new_expression=new_expression.replace(\"\\\"\\\"\",\"\\\"\")\n",
    "    new_expression=new_expression.replace(\",,\",\",\")\n",
    "    new_expression=new_expression.replace(\",]\",\"]\")\n",
    "    new_expression=new_expression.replace(\",)\",\")\")\n",
    "    new_expression=new_expression.replace(\"\\\"\\\"\",\"\\\"\")\n",
    "    step_3=new_expression\n",
    "    #No Longer needed due to fix in tests.\n",
    "\n",
    "    for match in re.finditer(repeated_set,new_expression):\n",
    "        #print \"This is the match\", match.group()\n",
    "        new_match=match.group().strip(\"set([\")\n",
    "        new_match=new_match.strip(\"])\")\n",
    "        new_match=\"set([\"+new_match+\"])\"\n",
    "        #print\n",
    "        #print \"This is the new match\", new_match\n",
    "        new_expression=new_expression.replace(match.group(),new_match)\n",
    "    \n",
    "    for match in re.finditer(set_in_set,new_expression):\n",
    "        new_match=match.group()\n",
    "        #print new_match.split(\"set([\")\n",
    "        blank, section_1,section_2=new_match.split(\"set([\")\n",
    "        section_2=section_2.strip(\"])\")\n",
    "        #print section_2\n",
    "        new_match=\"set([\"+section_1+section_2\n",
    "        new_expression=new_expression.replace(match.group(),new_match)\n",
    "    step_4=new_expression\n",
    "    isolated_start=\"^\\\"K[0-9]{5}\\\"\"\n",
    "    for match in re.findall(isolated_start,new_expression):\n",
    "        new_expression=new_expression.replace(match,\"(\"+match+\",)\")\n",
    "        \n",
    "    new_expression=\"(\"+new_expression+\")\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    if return_string:\n",
    "        return new_expression\n",
    "    try:\n",
    "        new_nesting=eval(new_expression)\n",
    "        return definition,new_nesting \n",
    "    \n",
    "\n",
    "        \n",
    "    except TypeError:\n",
    "        print definition\n",
    "        print \"0_1\",step_0_1\n",
    "        print \"0_2\",step_0_2\n",
    "        print \"Step 1:\", step_1\n",
    "        print \"Step 2:\", step_2\n",
    "        print \"Step 3:\", step_3\n",
    "        print \"Step 4:\", step_4\n",
    "        print \";\".join(set_matches)\n",
    "        print \"Type error\", new_expression\n",
    "        raise\n",
    "        \n",
    "    except SyntaxError:\n",
    "        print definition\n",
    "        print \"0_1\",step_0_1\n",
    "        print \"0_2\",step_0_2\n",
    "        print \"Step 1:\", step_1\n",
    "        print \"Step 2:\", step_2\n",
    "        print \"Step 3:\", step_3\n",
    "        print \"Step 4:\", step_4\n",
    "        print \";\".join(set_matches)\n",
    "        print \"Syntax error\", new_expression\n",
    "        raise\n",
    "        \n",
    "    except NameError:\n",
    "        print definition\n",
    "        print \"0_1\",step_0_1\n",
    "        print \"0_2\",step_0_2\n",
    "        print \"Step 1:\", step_1\n",
    "        print \"Step 2:\", step_2\n",
    "        print \"Step 3:\", step_3\n",
    "        print \"Step 4:\", step_4\n",
    "        print \";\".join(set_matches)\n",
    "        print \"NameError\", new_expression\n",
    "        raise\n",
    "        \n",
    "    return\n",
    "        \n",
    "     \n",
    "\n",
    "def alt_eval_kegg_bool(kegg_expr,ko_set):\n",
    "    '''\n",
    "    Description:\n",
    "        Evaluates a list of boolean expressions blocks to get a list of T, F results summarising the module completeness.\n",
    "    Input: \n",
    "        kegg_expr: List of sets\n",
    "            A kegg expression consting of KOs in nested tuples. eg, (KO1 ((KO2,KO3-KO4),KO5).\n",
    "            The separators represent the kegg boolean separators.\n",
    "\n",
    "        ko_set: set\n",
    "            The set of KOs to be evaluated for compelteness in this particular kegg expression.\n",
    "    Calls:\n",
    "        eval_kegg_bool: function\n",
    "            The workhorse of  this function - recursively evaluates each element in kegg_expr to\n",
    "            decide if it is actually true or false.\n",
    "    '''\n",
    "    n_elements=len(kegg_expr)\n",
    "    results_vec=[\"na\"]*n_elements\n",
    "    for i in xrange(0,n_elements,2):\n",
    "        current_element=kegg_expr[i]\n",
    "        if isinstance(current_element,tuple):\n",
    "            side_1_result=eval_kegg_bool(current_element,ko_set)\n",
    "        else:\n",
    "            side_1_result= len(ko_set & current_element)>0\n",
    "\n",
    "        full_result=side_1_result\n",
    "        #print full_result\n",
    "        results_vec[i]=full_result\n",
    "    #print results_vec\n",
    "    for i,element in enumerate(results_vec):\n",
    "        if not isinstance(element,bool):\n",
    "            results_vec[i]=kegg_expr[i]\n",
    "    \n",
    "    return (n_elements+1)/2,results_vec\n",
    "    \n",
    "    \n",
    "def eval_kegg_bool(kegg_expr,ko_set):\n",
    "    '''\n",
    "    Description:\n",
    "        A recursive implementation of the kegg boolean logic for evaluating based on a set of KOs if a module is complete.\n",
    "        If given a tuple it will recursively search down for more tuples and evaluate them at the lowest level to move up and\n",
    "        finally finish evaluating the complete block. \n",
    "    Input: \n",
    "        kegg_expr: List of sets\n",
    "            A kegg expression consting of KOs in nested tuples. eg, (KO1 ((KO2,KO3-KO4),KO5).\n",
    "            The separators represent the kegg boolean separators.\n",
    "        ko_set: set\n",
    "            The set of KOs to be evaluated for compelteness in this particular kegg expression.\n",
    "    Calls:\n",
    "        eval_kegg_bool: function\n",
    "            Evalutes logical KEGG blocks.\n",
    "    '''\n",
    "    n_elements=len(kegg_expr)\n",
    "    #vector=np.array([\"na\"]*((n_element+1)/2)-1)\n",
    "    for i in xrange(0,n_elements-1,2):\n",
    "        #print \"THe current kegg expression getting evaluated\", kegg_expr[i:i+3]\n",
    "        side_1,log_op,side_2=kegg_expr[i:i+3]\n",
    "        #print \"This is the logical operater being used\",log_op\n",
    "        if log_op==\" \" or log_op==\"+\":\n",
    "            #print \"Entering +  recursion\"\n",
    "            if isinstance(side_1,tuple):\n",
    "                side_1_result=eval_kegg_bool(side_1,ko_set)\n",
    "            else:\n",
    "                side_1_result= len(ko_set & side_1)>0\n",
    "            if isinstance(side_2,tuple):\n",
    "                side_2_result=eval_kegg_bool(side_2,ko_set)\n",
    "            else:\n",
    "                side_2_result=len(ko_set & side_2)>0\n",
    "            full_result=side_1_result and side_2_result\n",
    "            \n",
    "        elif \",\" in log_op:\n",
    "            #print \"Entering , recursion\"\n",
    "            if isinstance(side_1,tuple):\n",
    "                side_1_result=eval_kegg_bool(side_1,ko_set)\n",
    "            else:\n",
    "                side_1_result= len(ko_set & side_1)>0\n",
    "            if isinstance(side_2,tuple):\n",
    "                side_2_result=eval_kegg_bool(side_2,ko_set)\n",
    "            else:\n",
    "                side_2_result=len(ko_set & side_2)>0     \n",
    "            full_result=side_1_result or side_2_result\n",
    "        elif \"-\" in log_op:\n",
    "            #print \"Entering - recursion\"\n",
    "            if isinstance(side_1,tuple):\n",
    "                side_1_result=eval_kegg_bool(side_1,ko_set)\n",
    "            else:\n",
    "                side_1_result= len(ko_set & side_1)>0\n",
    "            if isinstance(side_2,tuple):\n",
    "                side_2_result=eval_kegg_bool(side_2,ko_set)\n",
    "            else:\n",
    "                side_2_result=len(ko_set & side_2)>0\n",
    "            full_result=side_1_result\n",
    "            \n",
    "        else:\n",
    "            print log_op, \"There seems to have been an error:\"\n",
    "        #print \"The result for side 1\", side_1_result, side_1\n",
    "        #print \"The result for side 2\", side_2_result, side_2\n",
    "    #print \"The final results being returned\", full_result\n",
    "    return full_result\n",
    "\n",
    "def block_level_completeness(results_vector,correct_partial,nested_descr,ko_set):\n",
    "    '''\n",
    "    Description:\n",
    "        Calculates the percent completness of a KEGG module in one of two ways. It either looks at the number of\n",
    "        logical blocks complete (block level completeness) or also adds a percentage adjustment for how complete the\n",
    "        incomplete blocks are.\n",
    "    Input:\n",
    "        results_vector: List of Booleans\n",
    "            A list containing the results of evaluating a kegg module KO hits as a boolean expression.\n",
    "        correct_partial: Boolean\n",
    "            Indicating whether to try and account for the partial completeness of some logical blocks.\n",
    "        nested_descr: nested tuple of sets\n",
    "            A logical grouping of KEGG blocks into tuples with sets of KOs as the lowermost elements.\n",
    "    Output:\n",
    "        completeness_perc:  float in [0,1]\n",
    "            Percent module completeness according to one of two methods.'''\n",
    "    if isinstance(nested_descr,set):\n",
    "        return len(nested_desc & ko_set) > 0\n",
    "    \n",
    "    keep_indices=True\n",
    "    log_blocks=make_logical_blocks(results_vector,keep_indices)\n",
    "    position_mapping=make_position_mapping(log_blocks)\n",
    "    if keep_indices:\n",
    "        log_blocks=extract_logical_values(log_blocks)\n",
    "    else:\n",
    "        pass\n",
    "    n_tot=len(log_blocks)\n",
    "    filled_blocks=[any(block) for block in log_blocks]\n",
    "    n_filled_blocks=sum(filled_blocks)\n",
    "    adjustment=[\"na\"]*len(log_blocks)\n",
    "    \n",
    "    if not correct_partial:\n",
    "        completeness_perc=float(n_filled_blocks)/n_tot\n",
    "        return completeness_perc\n",
    "    else:\n",
    "        for i,block in enumerate(log_blocks):\n",
    "            if not any(block):\n",
    "                n_max_hits=len(block)\n",
    "                running_total=0\n",
    "                for j,item in enumerate(block):\n",
    "                    if item:\n",
    "                        running_total+=1\n",
    "                    else:\n",
    "                        bool_index=position_mapping[i][j]\n",
    "                        #print \"The boolean index:\", bool_index\n",
    "                        #print position_mapping\n",
    "                        kegg_bool=nested_descr[bool_index]\n",
    "                        #print kegg_bool\n",
    "                        running_total+=module_completeness_proportion(kegg_bool,ko_set,correct_partial)\n",
    "                adjustment[i]=float(running_total)/n_max_hits\n",
    "            else:\n",
    "                adjustment[i]=1    \n",
    "        n_filled_blocks=sum(adjustment)\n",
    "        completeness_perc=float(n_filled_blocks)/n_tot\n",
    "        return completeness_perc\n",
    "    \n",
    "def extract_logical_values(logical_blocks):\n",
    "    return [[item[1] for item in block] for block in logical_blocks]\n",
    "\n",
    "def make_position_mapping(log_blocks):\n",
    "    mapping={}\n",
    "    for i, block in enumerate(log_blocks):\n",
    "        mapping[i]={j:item[0] for j,item in enumerate(block)}\n",
    "    #print mapping\n",
    "    return mapping\n",
    "\n",
    "def module_completeness_proportion(kegg_bool,ko_set,correct_partial):\n",
    "    '''Returns the completeness of the current kegg_boolean.\n",
    "    Input:\n",
    "        \n",
    "    Output:\n",
    "        \n",
    "    Calls:\n",
    "        block_level_completeness: Calculate the % of kegg blocks which are complete.'''\n",
    "    if isinstance(kegg_bool,set):\n",
    "        return len(kegg_bool & ko_set) > 0\n",
    "    #print \"Kegg bool:\",kegg_bool\n",
    "    #print \"ko_set:\",ko_set\n",
    "    n_el,results_vector=alt_eval_kegg_bool(kegg_bool,ko_set)\n",
    "    #print \"This is the result vector:\",results_vector\n",
    "    completeness_perc=block_level_completeness(results_vector,correct_partial,kegg_bool,ko_set)\n",
    "    \n",
    "    return completeness_perc\n",
    "    \n",
    "\n",
    "def make_logical_blocks(results_vector,keep_indices):\n",
    "    '''\n",
    "    Description:\n",
    "        Turn the uppermost level of results from a KEGG boolean into a series of logical blocks. I.e if I had \n",
    "        a vector [T and F and T or F or T] then the blocks formed will be [[T],[F],[T,F,T]]. \n",
    "    Input:\n",
    "        results_vector: List of Bools\n",
    "            A list containing the results of evaluating a kegg module KO hits as a boolean expression.\n",
    "    Output:\n",
    "        log_block: list of lists of bools\n",
    "            A list composed of the logical blocks needed to decide if a boolean is \"complete\".'''\n",
    "    \n",
    "    operator_set=set([\" \",\"-\",\",\",\"+\"])\n",
    "    log_blocks=[]\n",
    "    current_block=[]\n",
    "    previous_logical=\"\"\n",
    "    log_operators=[\" \",\",\",\"+\",\"-\"]\n",
    "    for i,item in enumerate(results_vector):\n",
    "        if item not in log_operators:\n",
    "            if not current_block:\n",
    "                if keep_indices:\n",
    "                    current_block.append((i,item))\n",
    "                else:\n",
    "                    current_block.append(item)\n",
    "#            elif i==(len(results_vector)-1):\n",
    "#                log_blocks.append(current_block)\n",
    "            else:\n",
    "                if previous_logical==\" \" or previous_logical==\"+\":\n",
    "                    log_blocks.append(current_block)\n",
    "                    if keep_indices:\n",
    "                        current_block=[(i,item)]\n",
    "                    else:\n",
    "                        current_block=[item]\n",
    "                elif previous_logical==\",\":\n",
    "                    #print item\n",
    "                    if keep_indices:\n",
    "                        current_block.append((i,item))\n",
    "                    else:\n",
    "                        current_block.append(item)\n",
    "                    #print current_block\n",
    "                elif previous_logical==\"-\":\n",
    "                    pass\n",
    "        else:\n",
    "            previous_logical=item\n",
    "    log_blocks.append(current_block)\n",
    "            \n",
    "    return log_blocks\n",
    "\n",
    "def test_all_local_modules(database_dir):\n",
    "    #758 comparisons are to be made.\n",
    "    completeness_dict={}\n",
    "    #Load all possible KOs\n",
    "    MO_KO_pairs=load_local_kegg_database_pairings(database_dir,[(\"Module\",\"orthology\")], False)[(\"Module\",\"orthology\")]\n",
    "    MO_KO_pairs={MO:set(KOs) for MO, KOs in MO_KO_pairs.iteritems()}\n",
    "    #Use this as the comparison set.\n",
    "    log_kegg_exprs=load_local_cleaned_definition_db(database_dir)\n",
    "    #Screen every single module for compelteness (should all be 1.0)\n",
    "    for module,expression in log_kegg_exprs.iteritems():\n",
    "        completeness_dict[module]=module_completeness_proportion(expression,MO_KO_pairs[module],True)\n",
    "        \n",
    "    \n",
    "    failures={Module:completeness for Module,completeness in completeness_dict.iteritems() if completeness<1}\n",
    "    print failures\n",
    "    \n",
    "    return completeness_dict\n",
    "\n",
    "genome_taxonomy=load_bin_names(shortened_tax_file)\n",
    "full_genome_taxonomy=load_bin_names(tax_file)\n",
    "uniq_genome_taxonomy=load_bin_names(uniq_tax_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metabolic Graphs + repeats enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import sklearn.manifold as manifold\n",
    "#from matplotlib import rcParams\n",
    "from mpl_toolkits.axes_grid import inset_locator\n",
    "#rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "def load_completeness_matrix(completeness_file):\n",
    "    df=pd.read_csv(completeness_file,index_col=[0,1],skipinitialspace=True,sep=\"\\t\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def perform_PCA(complete_matrix):\n",
    "    transposed_mat=pd.DataFrame.transpose(complete_matrix)\n",
    "    cov_frame=pd.DataFrame.cov(transposed_mat)\n",
    "    pca=PCA()\n",
    "    pca_values=pca.fit(cov_frame)\n",
    "    #print \"The tranposed matrix\", transposed_mat\n",
    "    new_values=pca_values.transform(transposed_mat)\n",
    "\n",
    "    obs_df=pd.DataFrame(new_values)\n",
    "    obs_df.index=transposed_mat.index\n",
    "    n_col=obs_df.shape[1]\n",
    "    col_names=[\"PC{0}\".format(j) for j in xrange(1,n_col+1)]\n",
    "    obs_df.columns=col_names\n",
    "    \n",
    "    #Extract the component values for each genome\n",
    "    \n",
    "    return obs_df, pca_values\n",
    "\n",
    "def perform_ISOMAP(complete_matrix):\n",
    "    \n",
    "    transposed_mat=pd.DataFrame.transpose(complete_matrix)\n",
    "\n",
    "    iso=manifold.Isomap(n_neighbors=3,n_components=5)\n",
    "\n",
    "    iso_model=iso.fit(transposed_mat)\n",
    "\n",
    "    #print dir(iso_model)\n",
    "    iso_val=iso_model.transform(transposed_mat)\n",
    "    iso_val=pd.DataFrame(iso_val)\n",
    "    iso_val.index=transposed_mat.index\n",
    "    #print iso_model.reconstruction_error()\n",
    "    #print iso_val.shape\n",
    "    n_col=iso_val.shape[1]\n",
    "    #print iso_val\n",
    "    col_names=[\"Dim{0}\".format(j) for j in xrange(1,n_col+1)]\n",
    "    iso_val.columns=col_names\n",
    "    \n",
    "    #print iso_val\n",
    "    \n",
    "    return iso_val\n",
    "\n",
    "def perform_MDS(complete_matrix):\n",
    "    \n",
    "    transposed_mat=pd.DataFrame.transpose(complete_matrix)\n",
    "    mds=manifold.MDS(n_components=3)\n",
    "    mds_val=mds.fit_transform(transposed_mat)\n",
    "    #mds_val=mds_model.transform(transposed_mat)\n",
    "    mds_val=pd.DataFrame(mds_val)\n",
    "    mds_val.index=transposed_mat.index\n",
    "    n_col=mds_val.shape[1]\n",
    "    col_names=[\"Dim{0}\".format(j) for j in xrange(1,n_col+1)]\n",
    "    mds_val.columns=col_names\n",
    "    \n",
    "    return mds_val\n",
    "\n",
    "def grouped_microbes(complete_matrix):\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "def pathway_to_modules(pathway_dict,database_dir):\n",
    "    links=load_local_kegg_database_pairings(database_dir,[(\"pathway\",\"module\")], False)[\"pathway\",\"module\"]\n",
    "    pathways={path:list(set(itertools.chain(*[links[egx] for egx in pathway if egx in links]))) for path, pathway in pathway_dict.iteritems()}\n",
    "    return pathways\n",
    "\n",
    "def plot_old_heatmap(subset_matrix,cmap,output_dir,path_name,core,colorbar_title=\"% pathway completeness\"): #perc_steps,\n",
    "    h2=sns.clustermap(subset_matrix, yticklabels=subset_matrix.index,row_cluster=False, col_cluster=False, xticklabels=subset_matrix.columns,cmap=cmap,linewidths=.5) #,cbar_kws={\"boundaries\":ticks}\n",
    "    h2.cax.set_visible(False)\n",
    "\n",
    "    for text in h2.ax_heatmap.get_yticklabels():\n",
    "        text.set_rotation('horizontal')\n",
    "    for text in h2.ax_heatmap.get_xticklabels():\n",
    "        text.set_rotation('vertical')\n",
    "        \n",
    "    #Remove top axes where colorbar is by default and where the column dendrogram normally goes\n",
    "    h2.fig.delaxes(h2.ax_col_dendrogram)\n",
    "    h2.fig.delaxes(h2.cax)\n",
    "    #Draw a colorbar where the row dendrogram normally is.\n",
    "    #Reposition and shrink axis to allow divided axes to sit in right place later\n",
    "    #h2.ax_row_dendrogram.set_position([0.16,0.125,0.2,0.6])\n",
    "    \n",
    "    #Break existing axis up into new subaxis\n",
    "    #divider=make_axes_locatable(h2.ax_row_dendrogram)\n",
    "    #new_cax=divider.append_axes(\"right\",size=\"50%\")\n",
    "    #New attempt at breaking up into a subaxis\n",
    "    inset_axes=inset_locator.inset_axes(h2.ax_row_dendrogram,width=\"20%\",height=\"50%\",loc=7) #10=centre, 7= centre right\n",
    "    #new_cax=divider.append_axes(\"right\",size=\"50%\")\n",
    "\n",
    "    #Plot colourbar on the sub axis\n",
    "    colourbar=mpl.colorbar.ColorbarBase(inset_axes,cmap=cmap,orientation=\"vertical\",drawedges=True)\n",
    "    #colourbar.ax.tick_params(labelsize=20)\n",
    "    colourbar.ax.yaxis.set_ticks_position('left') #Move ticks to left of colorbar\n",
    "    colourbar.set_label(colorbar_title)\n",
    "    colourbar.ax.yaxis.set_label_position('left') #Move title to left of colorbar\n",
    "    ylabel=colourbar.ax.get_yticklabels()\n",
    "    new_perc_labels=[str(perc) for perc in range(0,101,len(ylabel)-1)]\n",
    "    #print [type(tick) for tick in colourbar.ax.get_yticklabels()]\n",
    "    colourbar.ax.set_yticklabels(new_perc_labels)\n",
    "#    print dir(colourbar)\n",
    "    #h2.savefig(os.path.join(output_dir,'{}_grouped_microbes_modules_proportion.pdf'.format(path_name)))\n",
    "    formats=[\"eps\",\"svg\",\"pdf\",\"png\"]\n",
    "    core_name=os.path.join(output_dir,'{0}_{1}'.format(path_name,core))\n",
    "    for img_format in formats:\n",
    "        file_name=core_name+\".{0}\".format(img_format)\n",
    "        print \"Saving the file {0}\".format(file_name)\n",
    "        h2.savefig(file_name,format=img_format,bbox_inches=\"tight\")\n",
    "        \n",
    "    fix_eps(os.path.join(output_dir,'{0}_{1}.eps'.format(path_name,core)))\n",
    "    #h2.close()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def calc_best_ticks(top,perc_steps):\n",
    "    print type(top)\n",
    "    ticks=[round(top*perc,3) for perc in perc_steps]\n",
    "    return ticks\n",
    "\n",
    "def make_labels_from_ticks(ticks):\n",
    "    labels=['']*9\n",
    "    start=0\n",
    "    end=0\n",
    "    for i, tick in enumerate(ticks):\n",
    "        end=str(tick)\n",
    "        labels[i]=\"{0}-{1}\".format(start,end)\n",
    "    return labels\n",
    "        \n",
    "def plot_all_old_heatmaps(complete_matrix, output_dir,MO_pathways,bin_file_short,bin_file_long,core,transpose=False):\n",
    "    #MO_pathways=pathway_to_modules(pathways, database_dir)\n",
    "    \n",
    "    bin_name=load_bin_names(bin_file)\n",
    "    bin_name=defaultdict(lambda: \"No associated taxonomy.\",bin_name)\n",
    "    #i=0\n",
    "    bin_name_long=load_bin_names(bin_file)\n",
    "    bin_name=defaultdict(lambda: \"No associated taxonomy.\",bin_name)\n",
    "    \n",
    "    new_comp_mat=complete_matrix.copy()\n",
    "    new_comp_mat=new_comp_mat.sort_index(axis='columns')\n",
    "    new_comp_mat.index.names=['Module','ModuleDescription']\n",
    "    #print \"The new readable index\", new_comp_mat.index.get_level_values(0)\n",
    "    new_comp_mat=new_comp_mat.sort_index(axis='column',by=)\n",
    "    new_comp_mat=new_comp_mat.rename(columns=bin_name)\n",
    "    new_comp_mat=new_comp_mat.sort_index(axis='columns')\n",
    "#    cmap=\"Blues\"\n",
    "    #print new_comp_mat\n",
    "    col_vals=new_comp_mat.columns\n",
    "    new_order=list(col_vals[col_vals.isin([\"Porites lutea\",\"Symbiodinium strain C15\"])])+list(col_vals[~col_vals.isin([\"Porites lutea\",\"Symbiodinium strain C15\"])])\n",
    "    new_comp_mat=new_comp_mat[new_order]\n",
    "    for path_name,path_modules in MO_pathways.iteritems():\n",
    "        modules=set(new_comp_mat.index.get_level_values(0))\n",
    "        possible_modules=list(modules & set(path_modules))\n",
    "        #subset_matrix=new_comp_mat.loc[possible_modules,:]\n",
    "        selected_rows=pd.Series(new_comp_mat.index.get_level_values('Module')).isin(possible_modules)\n",
    "        selected_rows.index=new_comp_mat.index\n",
    "        #print \"The selected rows\", selected_rows\n",
    "        subset_matrix=new_comp_mat.loc[selected_rows]\n",
    "        #new_index={(multi_1,multi_2):multi_2 for multi_1,multi_2 in subset_matrix.index}\n",
    "        #print new_index\n",
    "        #subset_matrix=subset_matrix.rename(index=new_index)\n",
    "        #subset_matrix=subset_matrix.reset_index(1).reset_index(drop=True) #Move 2nd index col into data frame, drop remain index\n",
    "        subset_matrix.index=subset_matrix.index.droplevel('Module')\n",
    "        #print subset_matrix.index\n",
    "        if transpose:\n",
    "            subset_matrix=pd.DataFrame.transpose(subset_matrix)\n",
    "        plot_old_heatmap(subset_matrix,\"Blues\",output_dir, path_name,core)\n",
    "    #df.xs(1, level='A', drop_level=False)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def fix_eps(fpath):\n",
    "    \"\"\"Fix carriage returns in EPS files caused by Arial font.\"\"\"\n",
    "    txt = b\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        for line in f:\n",
    "            if b\"\\r\\rHebrew\" in line:\n",
    "                line = line.replace(b\"\\r\\rHebrew\", b\"Hebrew\")\n",
    "            txt += line\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        f.write(txt)\n",
    "\n",
    "\n",
    "def extract_matrix_subset(complete_matrix, modules):\n",
    "    '''Extracts specific rows.'''\n",
    "    subset_matrix=complete_matrix.loc[modules]\n",
    "    return subset_matrix\n",
    "\n",
    "\n",
    "def plot_scaling(MDS_data,y,x,file_name):\n",
    "    #sns.set_style('ticks')\n",
    "    sns.set(style='darkgrid')\n",
    "    fig,ax=plt.subplots(figsize=(15,10.5))\n",
    "    cur_plot=sns.regplot(x=x,y=y,data=MDS_data,fit_reg=False,scatter=True,scatter_kws={'s':30})\n",
    "\n",
    "    for index,row in MDS_data.iterrows():\n",
    "        cur_plot.text(row[x],row[y],row['group'],fontsize=20)\n",
    "    legend_labels=MDS_data.Tax.unique() #Unique tax strings\n",
    "    tax_ind_data=MDS_data[['Tax','group']].copy()\n",
    "    tax_ind_data=tax_ind_data.reset_index(drop=True)\n",
    "    tax_ind_data=tax_ind_data.set_index(['Tax'])\n",
    "    tax_ind_data=tax_ind_data.drop_duplicates()\n",
    "    tax_ind_data['group']=pd.to_numeric(tax_ind_data['group'])\n",
    "    tax_ind_data.sort_values(by='group',inplace=True)\n",
    "    tax_ind_data['group']=tax_ind_data['group'].apply(str)\n",
    "\n",
    "    labels=tax_ind_data['group']\n",
    "    description=tax_ind_data.index    \n",
    "    proxies=[create_proxy(item,'black') for item in labels]\n",
    "    lgd=plt.legend(proxies,description, numpoints=1,markerscale=2,loc=2,bbox_to_anchor=(1.0,1), borderaxespad=0.)\n",
    "    #cur_plot.savefig(os.path.join(output_dir,\"test_MDS.pdf\"),bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    plt.savefig(file_name+\".pdf\",format=\"pdf\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    plt.savefig(file_name+\".svg\",format=\"svg\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    plt.savefig(file_name+\".eps\",format=\"eps\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    plt.savefig(file_name+\".png\",forat=\"png\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    fix_eps(file_name+\".eps\")\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "def create_proxy(label,colour):\n",
    "    #from http://stackoverflow.com/questions/28739608/completely-custom-legend-in-matplotlib-python\n",
    "    #line=matplotlib.lines.Line2D([0],[0],linestyle='none',mfc=colour,mec='none',marker=r'$\\mathregular{{{}}}'.format(label))\n",
    "\n",
    "    line=matplotlib.lines.Line2D([0],[0],linestyle='none',mfc=colour,mec='none',marker=r'$\\mathregular{{{}}}$'.format(label))\n",
    "    return line\n",
    "\n",
    "\n",
    "\n",
    "def plot_scalings(orig_data,output_dir,tax_rel):\n",
    "    #remove module descriptions\n",
    "    original_data=orig_data.copy()\n",
    "    original_data.index=original_data.index.droplevel(1)\n",
    "    genome_ids=pd.Series(original_data.columns,index=original_data.columns).map(tax_rel)\n",
    "    #print genome_ids\n",
    "    genome_ids.sort_values(inplace=True)\n",
    "    labels,levels=pd.factorize(genome_ids)\n",
    "    #print labels\n",
    "    unique_IDS=pd.Series(labels,index=genome_ids.index).astype('string')\n",
    "    ID_df=pd.DataFrame(genome_ids)\n",
    "    ID_df['ID']=unique_IDS\n",
    "    ID_df.columns=['Tax','ID']\n",
    "    \n",
    "    PCA_data,PCA_model=perform_PCA(original_data)\n",
    "    ISOMAP_data=perform_ISOMAP(original_data)\n",
    "    MDS_data=perform_MDS(original_data)\n",
    "    \n",
    "    PCA_data['group']=ID_df['ID']\n",
    "    ISOMAP_data['group']=ID_df['ID']\n",
    "    MDS_data['group']=ID_df['ID']\n",
    "    \n",
    "    PCA_data['Tax']=ID_df['Tax']\n",
    "    ISOMAP_data['Tax']=ID_df['Tax']\n",
    "    MDS_data['Tax']=ID_df['Tax']\n",
    "    \n",
    "    pca_cols=PCA_data.columns[0:3]\n",
    "    isomap_cols=ISOMAP_data.columns[0:3]\n",
    "    mds_cols=MDS_data.columns[0:3]\n",
    "    \n",
    "    \n",
    "    PCA_data=PCA_data.sort_index(axis='columns')\n",
    "    ISOMAP_data=ISOMAP_data.sort_index(axis='columns')\n",
    "    MDS_data=MDS_data.sort_index(axis='columns')\n",
    "    \n",
    "    i=0\n",
    "    name=[\"PCA_Plot\",\"ISOMAP_plot\",'EUC_MDS_plots']\n",
    "    col_vals=[pca_cols,isomap_cols,mds_cols]\n",
    "    for data_values in itertools.chain([PCA_data,ISOMAP_data,MDS_data]):\n",
    "        file_name=os.path.join(output_dir, name[i])\n",
    "        for comp1,comp2 in itertools.combinations(col_vals[i],2):\n",
    "            lower_file_name=file_name+comp1+comp2\n",
    "            #print comp1, comp2\n",
    "            #print data_values\n",
    "            #data_values=data_values.sort_index(axis='rows')\n",
    "            #data_values=data_values.sort_index(axis='columns')\n",
    "            plot_scaling(data_values,comp1,comp2,lower_file_name)\n",
    "        i+=1\n",
    "    \n",
    "\n",
    "    file_name=os.path.join(output_dir,\"PCA_MDS_two_panel_plot\")\n",
    "    plot_scaling_two_panel_plot(MDS_data,PCA_data,file_name)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def plot_scaling_two_panel_plot(MDS_data,PCA_data,file_name):\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2)\n",
    "    sns.set(style='darkgrid')\n",
    "    plot_1=sns.regplot(x='Dim1',y='Dim2',data=MDS_data,fit_reg=False,scatter=True,scatter_kws={'s':30},ax=ax2)\n",
    "    for index,row in MDS_data.iterrows():\n",
    "        plot_1.text(row['Dim1'],row['Dim2'],row['group'],fontsize=10)\n",
    "\n",
    "    plot_2=sns.regplot(x='PC1',y='PC2',data=PCA_data,fit_reg=False,scatter=True,scatter_kws={'s':30},ax=ax1)\n",
    "\n",
    "    for index,row in PCA_data.iterrows():\n",
    "        plot_2.text(row['PC1'],row['PC2'],row['group'],fontsize=10)\n",
    "\n",
    "    legend_labels=MDS_data.Tax.unique() #Unique tax strings\n",
    "    tax_ind_data=MDS_data[['Tax','group']].copy()\n",
    "    tax_ind_data=tax_ind_data.reset_index(drop=True)\n",
    "    tax_ind_data=tax_ind_data.set_index(['Tax'])\n",
    "    tax_ind_data=tax_ind_data.drop_duplicates()\n",
    "    tax_ind_data['group']=pd.to_numeric(tax_ind_data['group'])\n",
    "    tax_ind_data.sort_values(by='group',inplace=True)\n",
    "    tax_ind_data['group']=tax_ind_data['group'].apply(str)\n",
    "\n",
    "    labels=tax_ind_data['group']\n",
    "    description=tax_ind_data.index    \n",
    "    proxies=[create_proxy(item,'black') for item in labels]\n",
    "    lgd=plt.legend(proxies,description, numpoints=1,markerscale=1.25,loc=2,bbox_to_anchor=(1.0,1), borderaxespad=0.,prop={'size':8})\n",
    "    #plt.close()\n",
    "    plt.savefig(file_name+\".pdf\",format=\"pdf\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    plt.savefig(file_name+\".svg\",format=\"svg\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    plt.savefig(file_name+\".eps\",format=\"eps\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    plt.savefig(file_name+\".png\",forat=\"png\",bbox_extra_artists=(lgd,),bbox_inches='tight')\n",
    "    fix_eps(file_name+\".eps\")\n",
    "    return None\n",
    "def plotting_wf(completes_matrix, tax_file, relevant_pathways):\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_completeness_matrix(completeness_file):\n",
    "    df=pd.read_csv(completeness_file,index_col=[0,1],skipinitialspace=True,sep=\"\\t\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_euk_repeat_data_matrix(euk_repeat_file):\n",
    "    df=pd.read_csv(euk_repeat_file,index_col=[0,1,2],skipinitialspace=True,sep=\"\\t\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_euk_repeat_heatmaps(euk_repeat_matrix,output_dir,cmap,names):\n",
    "    repeat_data=euk_repeat_matrix.copy()\n",
    "    repeat_data.index=repeat_data.index.droplevel(0)\n",
    "    repeat_data=repeat_data.loc[:,(repeat_data!=0).any(axis=0)] #Remove all columns with no hits.\n",
    "    repeat_data=repeat_data.sort_index(axis='rows')\n",
    "    #names=[\"TPR\",'Ank__VCBS__T2SSE__TAL_effector','W']\n",
    "    column_dict={}\n",
    "    n_cols=repeat_data.shape[1]\n",
    "    other_columns=pd.Series([False]*n_cols,index=repeat_data.columns)\n",
    "    for name in names:\n",
    "        options=name.split(\"__\")\n",
    "        empty_series=pd.Series([False]*n_cols,index=repeat_data.columns)\n",
    "        for option in options:\n",
    "            empty_series=empty_series | pd.Series(repeat_data.columns.str.startswith(option),index=repeat_data.columns)\n",
    "        #log_series=pd.Series(repeat_data.columns.str.startswith(name),index=repeat_data.columns)\n",
    "        #log_series.index=repeat_data.columns\n",
    "        #print len(log_series)\n",
    "        column_dict[name]=empty_series\n",
    "\n",
    "    for bool_series in column_dict.itervalues():\n",
    "        other_columns=other_columns | bool_series\n",
    "    other_columns=~other_columns\n",
    "\n",
    "    #print len(other_columns)\n",
    "\n",
    "    column_dict[\"other\"]=other_columns\n",
    "    cmap=\"Blues\"\n",
    "    for item_name,logical_series in column_dict.iteritems():\n",
    "        #print repeat_data.shape[1]\n",
    "        #print item_name, logical_series\n",
    "        subset_matrix=repeat_data.ix[:,logical_series]\n",
    "        print subset_matrix.shape[1]\n",
    "        #subset_matrix=pd.DataFrame.transpose(subset_matrix)\n",
    "        plot_heatmap(subset_matrix,cmap,output_dir,item_name,\"Euk_repeat_Hmm_searches\",cluster_rows=False)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def split_df_by_column_names(df,names):\n",
    "    column_dict={}\n",
    "    sub_df_dict={}\n",
    "    n_cols=df.shape[1]\n",
    "    other_columns=pd.Series([False]*n_cols,index=df.columns)\n",
    "    for name in names:\n",
    "        options=name.split(\"__\")\n",
    "        empty_series=pd.Series([False]*n_cols,index=df.columns)\n",
    "        for option in options:\n",
    "            empty_series=empty_series | pd.Series(df.columns.str.startswith(option),index=df.columns)\n",
    "        #log_series=pd.Series(repeat_data.columns.str.startswith(name),index=repeat_data.columns)\n",
    "        #log_series.index=repeat_data.columns\n",
    "        #print len(log_series)\n",
    "        column_dict[name]=empty_series\n",
    "\n",
    "    for bool_series in column_dict.itervalues():\n",
    "        other_columns=other_columns | bool_series\n",
    "    other_columns=~other_columns\n",
    "    column_dict[\"other\"]=other_columns\n",
    "    for item_name,logical_series in column_dict.iteritems():\n",
    "        #print repeat_data.shape[1]\n",
    "        #print item_name, logical_series\n",
    "        sub_df_dict[item_name]=df.ix[:,logical_series]\n",
    "    return sub_df_dict\n",
    "\n",
    "\n",
    "def split_df_by_row_names(df,names):\n",
    "    rows_dict={}\n",
    "    sub_df_dict={}\n",
    "    n_cols=df.shape[0]\n",
    "    other_rows=pd.Series([False]*n_cols,index=df.index)\n",
    "    for name in names:\n",
    "        options=name.split(\"__\")\n",
    "        empty_series=pd.Series([False]*n_cols,index=df.index)\n",
    "        for option in options:\n",
    "            empty_series=empty_series | pd.Series(df.index.str.startswith(option),index=df.index)\n",
    "        #log_series=pd.Series(repeat_data.columns.str.startswith(name),index=repeat_data.columns)\n",
    "        #log_series.index=repeat_data.columns\n",
    "        #print len(log_series)\n",
    "        rows_dict[name]=empty_series\n",
    "\n",
    "    for bool_series in rows_dict.itervalues():\n",
    "        other_rows=other_rows | bool_series\n",
    "    other_rows=~other_rows\n",
    "    rows_dict[\"other\"]=other_rows\n",
    "    for item_name,logical_series in rows_dict.iteritems():\n",
    "        #print repeat_data.shape[1]\n",
    "        #print item_name, logical_series\n",
    "        sub_df_dict[item_name]=df.ix[logical_series,:]\n",
    "    return sub_df_dict\n",
    "\n",
    "def split_df_by_column_values(df, breakpoint_values):\n",
    "    col_max=df.max(axis=0) #Get column maximum\n",
    "    #print type(col_max)\n",
    "    i=0\n",
    "    start=0\n",
    "    split_dfs={}\n",
    "    for j in xrange(0,len(breakpoint_values)+1):\n",
    "        if j!=len(breakpoint_values):\n",
    "            end=breakpoint_values[j]\n",
    "            df_id=\"{0}<=X<{1}\".format(start,end)\n",
    "            des_columns=(start<=col_max) & (end>col_max)\n",
    "            #print des_columns\n",
    "            if des_columns.any():\n",
    "                split_dfs[df_id]=df.ix[:,des_columns]\n",
    "            start=breakpoint_values[j]\n",
    "        else:\n",
    "            df_id=\"{0}<=X\".format(start)\n",
    "            des_columns=(start<=col_max)\n",
    "            #print des_columns\n",
    "            if des_columns.any():\n",
    "                split_dfs[df_id]=df.ix[:,des_columns]\n",
    "    \n",
    "    return split_dfs\n",
    "\n",
    "def split_df_by_row_values(df, breakpoint_values):\n",
    "    col_max=df.max(axis=1) #Get column maximum\n",
    "    #print type(col_max)\n",
    "    i=0\n",
    "    start=0\n",
    "    split_dfs={}\n",
    "    for j in xrange(0,len(breakpoint_values)+1):\n",
    "        if j!=len(breakpoint_values):\n",
    "            end=breakpoint_values[j]\n",
    "            df_id=\"{0}<=X<{1}\".format(start,end)\n",
    "            des_columns=(start<=col_max) & (end>col_max)\n",
    "            #print des_columns\n",
    "            if des_columns.any():\n",
    "                split_dfs[df_id]=df.ix[des_columns,:]\n",
    "            start=breakpoint_values[j]\n",
    "        else:\n",
    "            df_id=\"{0}<=X\".format(start)\n",
    "            des_columns=(start<=col_max)\n",
    "            #print des_columns\n",
    "            if des_columns.any():\n",
    "                split_dfs[df_id]=df.ix[:,des_columns]\n",
    "            \n",
    "    return split_dfs\n",
    "\n",
    "def any_greater_than(matrix,value):\n",
    "    new_matrix=matrix.copy()\n",
    "    new_matrix=new_matrix.loc[(new_matrix>=value).any(axis=1),:]\n",
    "    return new_matrix\n",
    "\n",
    "def load_counts(file_name):\n",
    "    df=pd.read_csv(file_name,sep=\"\\t\",index_col=0,header=None)\n",
    "    df.columns=[\"TotalGene#\"]\n",
    "    df.index.name=\"Genome_id\"\n",
    "    return df\n",
    "\n",
    "def discretize_matrix(matrix,ticks):\n",
    "    discretised_matrix=matrix.copy()\n",
    "    num_ints=len(ticks)\n",
    "    tick_labels=make_tick_labels(ticks)\n",
    "    disc_val=0\n",
    "    for i,j in zip(ticks,ticks[1:]):\n",
    "        discretised_matrix[(i<=matrix) & (matrix<j)]=disc_val\n",
    "        disc_val+=1\n",
    "    discretised_matrix[(matrix>j)]=disc_val\n",
    "    return discretised_matrix\n",
    "        \n",
    "        \n",
    "def make_tick_labels(ticks):\n",
    "    lables=[]\n",
    "    for i,j in zip(ticks,ticks[1:]):\n",
    "        lables.append(\"{0}-{1}\".format(i,j))\n",
    "    lables.append(\">={0}\".format(ticks[-1]))\n",
    "    return lables\n",
    "\n",
    "def plot_discrete_heatmap(matrix,ticks,cmap,output_dir,path_name,core,colorbar_title=\"% Relative abundance\",labels=None):\n",
    "    discrete_data=discretize_matrix(matrix,ticks)\n",
    "    #discrete_data=discrete_data/max(discrete_data.max(axis=1))\n",
    "    if isinstance(labels,type(None)):\n",
    "        tick_labels=make_tick_labels(ticks)\n",
    "    else:\n",
    "        tick_labels=labels\n",
    "    n_ticks=len(ticks)\n",
    "    n_colours=n_ticks\n",
    "    #cmap=\"Blues\"\n",
    "    cmap=sns.cubehelix_palette(len(ticks),rot=-0.3)\n",
    "    cmap[0]=[1]*3\n",
    "    #test_cmap=sns.cubehelix_palette(len(ticks),rot=-0.3,as_cmap=True)\n",
    "    cmap=[tuple(cmapped) for cmapped in cmap]\n",
    "    cm = LinearSegmentedColormap.from_list(\n",
    "        \"CubeHelix_Discrete\", colors=cmap, N=n_ticks)\n",
    "    h2=sns.clustermap(discrete_data, yticklabels=discrete_data.index,\\\n",
    "                      row_cluster=False, col_cluster=False, \\\n",
    "                      xticklabels=discrete_data.columns,\\\n",
    "                      cmap=cm,linewidths=.5) #,cbar_kws={\"boundaries\":ticks}\n",
    "    h2.cax.set_visible(False)\n",
    "    #plt.yticks(rotation=0)\n",
    "    #plt.xticks(rotation=45)\n",
    "    #plt.tight_layout()\n",
    "    #colorbar = h2.collections[0].colorbar\n",
    "    #colorbar.set_ticks(ticks)\n",
    "    #colorbar.set_ticklabels(labels)\n",
    "\n",
    "    #h2.ax_heatmap.set_title(path_name)\n",
    "    for text in h2.ax_heatmap.get_yticklabels():\n",
    "        text.set_rotation('horizontal')\n",
    "    for text in h2.ax_heatmap.get_xticklabels():\n",
    "        text.set_rotation('vertical')\n",
    "\n",
    "    h2.fig.delaxes(h2.ax_col_dendrogram)\n",
    "    h2.fig.delaxes(h2.cax)\n",
    "\n",
    "    #Define a colour bar in the appropiate way.\n",
    "    #h2.ax_row_dendrogram.set_position([0.16,0.125,0.2,0.6])\n",
    "    #divider=make_axes_locatable(h2.ax_row_dendrogram)\n",
    "    #new_cax=divider.append_axes(\"right\",size=\"50%\")\n",
    "    inset_axes=inset_locator.inset_axes(h2.ax_row_dendrogram,width=\"20%\",height=\"50%\",loc=7) #10=centre, 7= centre right\n",
    "    colourbar=mpl.colorbar.ColorbarBase(inset_axes,cmap=cm,orientation=\"vertical\",drawedges=True)\n",
    "    #colourbar.ax.tick_params(labelsize=20)\n",
    "    colourbar.ax.yaxis.set_ticks_position('left')\n",
    "    tick_position=np.linspace(0.5, n_colours-0.5, n_colours)\n",
    "    tick_position=tick_position/n_colours#-float(0.25)/(n_ticks)\n",
    "    colourbar.set_label(colorbar_title)\n",
    "    colourbar.ax.yaxis.set_label_position('left') #Move title to left of colorbar\n",
    "    #print tick_position\n",
    "    #print tick_labels\n",
    "    colourbar.set_ticks(tick_position)\n",
    "    colourbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "    formats=[\"eps\",\"svg\",\"pdf\",\"png\"]\n",
    "    core_name=os.path.join(output_dir,'{0}_{1}'.format(path_name,core))\n",
    "    for img_format in formats:\n",
    "        file_name=core_name+\".{0}\".format(img_format)\n",
    "        print \"Saving the file {0}\".format(file_name)\n",
    "        h2.savefig(file_name,format=img_format,bbox_inches=\"tight\")\n",
    "        \n",
    "    fix_eps(os.path.join(output_dir,'{0}_{1}.eps'.format(path_name,core)))\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_new_multiheader_hmm_searches(hmm_multiind_file,genome_taxonomy):\n",
    "    df=pd.read_csv(hmm_multiind_file,index_col=[0,1],header=[0,1],sep=\"\\t\",skipinitialspace=True)\n",
    "    df=df.fillna(0)\n",
    "    df=df.loc[:,(df!=0).any(axis=0)] #Remove all columns with no hits.\n",
    "    df.index=df.index.droplevel(1)\n",
    "    df.columns=df.columns.droplevel(0)\n",
    "    df.index=df.index.str.replace(\"plut.*\",\"coral\").str.replace(\"SymbC15.*\",\"SymbC15\")\n",
    "    df=df.rename(index=genome_taxonomy)\n",
    "    df=df.sort_index(axis='rows')\n",
    "    return df\n",
    "\n",
    "def create_multiheader_hmm_discrete_heatmap(hmm_matrix, output_dir,cmap,row_splits,matrix_ticks,core_name,labels):\n",
    "    sep_assemblies=split_df_by_row_names(hmm_matrix,['filtered__unfiltered'])\n",
    "    sep_assemblies=defaultdict(dict,sep_assemblies)\n",
    "    #for name, df in sep_assemblies.items():\n",
    "    #    sep_assemblies[name]=split_df_by_column_values(df, column_splits[name])\n",
    "    \n",
    "    for name, df in sep_assemblies.iteritems():\n",
    "        item_name=name\n",
    "        plot_discrete_heatmap(df,matrix_ticks[name],cmap,output_dir,item_name,core_name,labels[name])\n",
    "    return None\n",
    "\n",
    "def load_counts(file_name):\n",
    "    df=pd.read_csv(file_name,sep=\"\\t\",index_col=0,header=None)\n",
    "    df.columns=[\"TotalGene#\"]\n",
    "    df.index.name=\"Genome_id\"\n",
    "    return df\n",
    "\n",
    "def normalise_euk_data(euk_repeat_file, gene_totals_file,uniq_tax_id_file):\n",
    "    repeat_data=load_euk_repeat_data_matrix(euk_repeat_file)\n",
    "    repeat_data.index=repeat_data.index.droplevel(\"Taxonomy\")\n",
    "    repeat_data.index=repeat_data.index.droplevel('Gene_name')\n",
    "    gene_totals=load_counts(gene_totals_file)\n",
    "    repeat_data=repeat_data.loc[:,(repeat_data!=0).any(axis=0)] #Remove all columns with no hits.\n",
    "    repeat_data[repeat_data>0]=1\n",
    "    repeat_data[repeat_data==0]=0\n",
    "    #print repeat_data\n",
    "    repeat_data=repeat_data.groupby(level=0).sum()\n",
    "    #print repeat_data\n",
    "    #print gene_totals.index\n",
    "    repeat_data=repeat_data.divide(gene_totals.iloc[:,0]/100, axis='index')\n",
    "    repeat_data=repeat_data.sort_index(axis='rows')\n",
    "    #print repeat_data\n",
    "    #names=[\"TPR\",'Ank__VCBS__T2SSE__TAL_effector','W']\n",
    "    taxa=load_bin_names(uniq_tax_file)\n",
    "    taxa=defaultdict(lambda : \"No Associated taxonomy\", taxa)\n",
    "    repeat_data.index=repeat_data.index.map(lambda x: taxa[x])\n",
    "    sub_dfs=split_df_by_column_names(repeat_data,[\"Ank\",\"WD40\",\"VCBS\"])\n",
    "    sub_dfs[\"Ank\"]=sub_dfs[\"Ank\"].loc[:,\"Ank\"]\n",
    "    sub_dfs[\"WD40\"]=sub_dfs[\"WD40\"].loc[:,\"WD40\"]\n",
    "    sub_dfs[\"VCBS\"]=sub_dfs[\"VCBS\"].loc[:,\"VCBS\"]\n",
    "    del sub_dfs[\"other\"]\n",
    "    for name,series in sub_dfs.items():\n",
    "        sub_dfs[name]=sub_dfs[name].sort_index()\n",
    "    return sub_dfs\n",
    "\n",
    "def create_gene_perc_bar_charts(sub_dfs,cmap,output_dir,core):\n",
    "    plt.rcParams['figure.figsize']=(8,5)\n",
    "    for protein_repeat,data_series in sub_dfs.iteritems():\n",
    "\n",
    "        fig,ax=plt.subplots()\n",
    "        sns.barplot(x=data_series.index,y=data_series.values,ax=ax,color = \"grey\")\n",
    "        locs,labels=plt.xticks()\n",
    "        plt.setp(labels,rotation=90)\n",
    "        plt.title(protein_repeat)\n",
    "        plt.ylabel(\"% of all genes hit\")\n",
    "        if \"ank\" in protein_repeat.lower():\n",
    "            plt.axhline(y=0.25, xmin=0, xmax=1, hold=None,color='black',linewidth=2)\n",
    "            #plt.axhline(y=0.20, xmin=0, xmax=1, hold=None,color='black',linewidth=2)\n",
    "        fig.savefig(os.path.join(output_dir,'{0}_{1}.eps'.format(protein_repeat,core)),format=\"eps\",bbox_inches=\"tight\")\n",
    "        fig.savefig(os.path.join(output_dir,'{0}_{1}.svg'.format(protein_repeat,core)),format=\"svg\",bbox_inches=\"tight\")\n",
    "        fig.savefig(os.path.join(output_dir,'{0}_{1}.pdf'.format(protein_repeat,core)),format=\"pdf\",bbox_inches=\"tight\")\n",
    "        fig.savefig(os.path.join(output_dir,'{0}_{1}.png'.format(protein_repeat,core)),format=\"png\",bbox_inches=\"tight\")\n",
    "        fix_eps(os.path.join(output_dir,'{0}_{1}.eps'.format(protein_repeat,core)))\n",
    "        #plt.show()\n",
    "    return\n",
    "\n",
    "def histogram_figures(euk_repeat_file, gene_totals_file,uniq_tax_id_file,output_dir):\n",
    "    sub_dfs=normalise_euk_data(euk_repeat_file, gene_totals_file,uniq_tax_id_file)\n",
    "    create_gene_perc_bar_charts(sub_dfs,\"Blacks\",output_dir,\"perc_gene_hits_barchart\")\n",
    "    return\n",
    "    \n",
    "def grepl(iterable,pattern):\n",
    "    return [bool(re.search(pattern,x)) for x in iterable]\n",
    "\n",
    "def process_binning_improvement_data(binning_data_file):\n",
    "    binning_improvement_data=pd.read_csv(binning_data_file,index_col=[0,1],sep=\",\")\n",
    "    binning_improvement_data.index=binning_improvement_data.index.droplevel(\"Steve_taxonomy\")\n",
    "    \n",
    "    completeness=binning_improvement_data.iloc[:,0::2]\n",
    "    contamination=binning_improvement_data.iloc[:,1::2]\n",
    "    completeness=pd.melt(completeness,id_vars=[\"Final_Completeness\"],var_name=\"binning_algorithm\")\n",
    "    completeness.iloc[:,1]=completeness.iloc[:,1].str.replace(\"_completeness\",\"\")\n",
    "    completeness[\"%Improvement\"]=completeness[\"Final_Completeness\"]-completeness[\"value\"]\n",
    "    \n",
    "    contamination=pd.melt(contamination,id_vars=[\"Final_Contamination\"],var_name=\"binning_algorithm\")\n",
    "    contamination.iloc[:,1]=contamination.iloc[:,1].str.replace(\"_contamination\",\"\")\n",
    "    contamination[\"%Improvement\"]=contamination[\"value\"]-contamination[\"Final_Contamination\"]\n",
    "\n",
    "    contamination[\"Measure\"]=\"Contamination\"\n",
    "#contamination\n",
    "    completeness[\"Measure\"]=\"Completeness\"\n",
    "\n",
    "    output_data=pd.concat([completeness,contamination])\n",
    "    #output_dict={}\n",
    "    #output_dict[\"completeness\"]=completeness\n",
    "    #output_dict[\"contamination\"]=contamination\n",
    "    return output_data#ict\n",
    "\n",
    "def plot_completeness_contamination(input_data,output_dir,core):\n",
    "    #for item, values in input_data.iteritems():\n",
    "    fig,ax=plt.subplots()\n",
    "    sns.barplot(x=\"binning_algorithm\",y=\"%Improvement\",data=input_data,hue=\"Measure\",ax=ax)\n",
    "\n",
    "    item=\"Binning_statistics\"\n",
    "    fig.savefig(os.path.join(output_dir,'{0}_{1}.eps'.format(item,core)),format=\"eps\",bbox_inches=\"tight\")\n",
    "    fig.savefig(os.path.join(output_dir,'{0}_{1}.svg'.format(item,core)),format=\"svg\",bbox_inches=\"tight\")\n",
    "    fig.savefig(os.path.join(output_dir,'{0}_{1}.pdf'.format(item,core)),format=\"pdf\",bbox_inches=\"tight\")\n",
    "    fig.savefig(os.path.join(output_dir,'{0}_{1}.png'.format(item,core)),format=\"png\",bbox_inches=\"tight\")\n",
    "    fix_eps(os.path.join(output_dir,'{0}_{1}.eps'.format(item,core)))\n",
    "def create_improvement_barplots(binning_data_file,output_dir):\n",
    "    input_data=process_binning_improvement_data(binning_data_file)\n",
    "    plot_completeness_contamination(input_data,output_dir,\"_improvements\")\n",
    "    return\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "euk_repeat_file=os.path.join(*[core,g_drive,\"HMM_searches\",\"Symbioses_test\",\"euk_repeat_results\",\"hmm_hits_per_gene_per_genome.tsv\"])\n",
    "\n",
    "gene_totals_file=os.path.join(*[core,g_drive,\"HMM_searches\",\"gene_counts.tsv\"])\n",
    "\n",
    "histogram_figures(euk_repeat_file, gene_totals_file,uniq_tax_file,plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repeat_data=load_euk_repeat_data_matrix(euk_repeat_file)\n",
    "repeat_data.index=repeat_data.index.droplevel(\"Taxonomy\")\n",
    "repeat_data.index=repeat_data.index.droplevel('Gene_name')\n",
    "gene_totals=load_counts(gene_totals_file)\n",
    "repeat_data=repeat_data.loc[:,(repeat_data!=0).any(axis=0)] #Remove all columns with no hits.\n",
    "repeat_data[repeat_data>0]=1\n",
    "repeat_data[repeat_data==0]=0\n",
    "#print repeat_data\n",
    "repeat_data=repeat_data.groupby(level=0).sum()\n",
    "#print repeat_data\n",
    "#print gene_totals.index\n",
    "repeat_data=repeat_data.divide(gene_totals.iloc[:,0]/100, axis='index')\n",
    "repeat_data=repeat_data.sort_index(axis='rows')\n",
    "#print repeat_data\n",
    "#names=[\"TPR\",'Ank__VCBS__T2SSE__TAL_effector','W']\n",
    "taxa=load_bin_names(uniq_tax_file)\n",
    "taxa=defaultdict(lambda : \"No Associated taxonomy\", taxa)\n",
    "repeat_data.index=repeat_data.index.map(lambda x: taxa[x])\n",
    "sub_dfs=split_df_by_column_names(repeat_data,[\"Ank\",\"WD40\",\"VCBS\"])\n",
    "sub_dfs[\"Ank\"]=sub_dfs[\"Ank\"].loc[:,\"Ank\"]\n",
    "sub_dfs[\"WD40\"]=sub_dfs[\"WD40\"].loc[:,\"WD40\"]\n",
    "sub_dfs[\"VCBS\"]=sub_dfs[\"VCBS\"].loc[:,\"VCBS\"]\n",
    "del sub_dfs[\"other\"]\n",
    "for name,series in sub_dfs.items():\n",
    "    sub_dfs[name]=sub_dfs[name].sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "improvements_file=os.path.join(*[core,g_drive, \"BetterBins\",\"GTDB_all_bin_comparison_csv_file.csv\"])\n",
    "\n",
    "create_improvement_barplots(improvements_file, plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print m\n",
    "\n",
    "plot_scalings(m,plots_dir,genome_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_output_dir=os.path.join(*[core,g_drive,\"metabolic_analysis\",\"enriched_hits\"])\n",
    "m=pd.read_csv(os.path.join(enrich_output_dir,\"Genome_module_completeness_matrix_all_orgs.tsv\"),\\\n",
    "              index_col=[0,1],skipinitialspace=True,sep=\"\\t\")\n",
    "\n",
    "subset_dir=os.path.join(*[output_dir,\"subset_data\"])\n",
    "test_data=os.path.join(*[subset_dir,\"nitrogen-sulfur-fatty_acid-photosynthesis.tsv\"])\n",
    "n=pd.read_csv(test_data,sep=\"\\t\",index_col=[0])\n",
    "\n",
    "#Simplify the string names\n",
    "Fullregex1=\"(two-component regulatory system|transport system)\"\n",
    "Fullregex2=\"(.*,) (?!(ammonia|sulfate|nitrate|thiosulfate))\"\n",
    "#print p.index\n",
    "new_levels=m.index.get_level_values(1).str.replace(Fullregex1,\"\").str.replace(Fullregex2,\"\")\n",
    "#print len(p.index)\n",
    "#print new_levels\n",
    "\n",
    "new_index=pd.MultiIndex.from_tuples(list(zip(*[m.index.levels[0],new_levels])))\n",
    "\n",
    "#print len(new_index)\n",
    "#print len(p.index.levels[0])\n",
    "##print len(new_levels)\n",
    "#print p.index.get_level_values(1)\n",
    "m.index=new_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subdfs(complete_matrix, output_dir,pathways,bin_file):\n",
    "    MO_pathways=pathway_to_modules(pathways, database_dir)\n",
    "    bin_name=load_bin_names(bin_file)\n",
    "    bin_name=defaultdict(lambda: \"No associated taxonomy.\",bin_name)\n",
    "    #i=0\n",
    "    new_comp_mat=complete_matrix.copy()\n",
    "    new_comp_mat=new_comp_mat.sort_index(axis='columns')\n",
    "    new_comp_mat.index.names=['Module','ModuleDescription']\n",
    "    #print \"The new readable index\", new_comp_mat.index.get_level_values(0)\n",
    "    new_comp_mat=new_comp_mat.rename(columns=bin_name)\n",
    "    new_comp_mat=new_comp_mat.sort_index(axis='columns')\n",
    "    cmap=\"Blues\"\n",
    "    #print new_comp_mat\n",
    "    for path_name,path_modules in MO_pathways.iteritems():\n",
    "        modules=set(new_comp_mat.index.get_level_values(0))\n",
    "        possible_modules=list(modules & set(path_modules))\n",
    "        #subset_matrix=new_comp_mat.loc[possible_modules,:]\n",
    "        selected_rows=pd.Series(new_comp_mat.index.get_level_values('Module')).isin(possible_modules)\n",
    "        selected_rows.index=new_comp_mat.index\n",
    "        #print \"The selected rows\", selected_rows\n",
    "        subset_matrix=new_comp_mat.loc[selected_rows].copy()\n",
    "        #new_index={(multi_1,multi_2):multi_2 for multi_1,multi_2 in subset_matrix.index}\n",
    "        #print new_index\n",
    "        #subset_matrix=subset_matrix.rename(index=new_index)\n",
    "        #subset_matrix=subset_matrix.reset_index(1).reset_index(drop=True) #Move 2nd index col into data frame, drop remain index\n",
    "        subset_matrix.index=subset_matrix.index.droplevel('Module')\n",
    "        #print subset_matrix.index\n",
    "        output_file=os.path.join(output_dir,path_name+\".tsv\")\n",
    "        subset_matrix.to_csv(output_file,sep=\"\\t\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baker\\\\Google Drive\\\\Honours\\\\metabolic_analysis'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MO_pathways=pathway_to_modules(pathways,database_dir)\n",
    "\n",
    "operator_module_dict={\"Remove\":{'amino-acids':[],\n",
    " 'CationicAntiomicrobialPeptide_CAMP_resistance':[],\n",
    " 'Vancomycin_Beta-lactamResistance':[],\n",
    " 'Glycosaminoglycan degradation & Synthesis':[],\n",
    " 'N-Glycan biosynthesis':[],\n",
    " 'two-component':[\"M00315\",\"M00449\",\"M00455\",\"M00459\",\"M00461\",\\\n",
    "                  \"M00462\",\"M00467\",\"M00468\",\"M00471\",\"M00472\",\\\n",
    "                  \"M00474\",\"M00475\",\"M00480\",\"M00483\",\"M00485\",\\\n",
    "                  \"M00489\",\"M00492\",\"M00501\",\"M00502\",\"M00504\",\\\n",
    "                  \"M00506\",\"M00507\",\"M00510\",\"M00512\",\"M00513\",\\\n",
    "                  \"M00516\",\"M00517\",\"M00645\",\"M00651\",\"M00716\",\\\n",
    "                  \"M00770\",\"M00772\",\"M00646\",\"M00648\",\"M00511\",\\\n",
    "                  \"M00477\"],\n",
    " 'vitamins&cofactors':[\"M00141\",\"M00140\",\"M00622\",\"M00811\"],\n",
    " 'AminoAcidMetabolism':[\"M00032\",\"M00135\",\"M00136\",\"M00047\",\\\n",
    "                        \"M00134\",\"M00133\",\"M00044\",\"M00533\",\\\n",
    "                        \"M00043\",\"M00042\",\"M00545\",\"M00350\",\\\n",
    "                        \"M00037\",\"M00741\",\"M00741\",\"M00033\",\\\n",
    "                        \"M00034\",\"M00035\",\"M00368\",\"M00036\",\\\n",
    "                        \"M00038\",\"M00045\"],\n",
    "                                \n",
    " 'ABC transporters':[\"M00185\",\"M00191\",\"M00762\",\"M00739\",\\\n",
    "                     \"M00635\",\"M00607\",\"M00606\",\"M00605\",\\\n",
    "                     \"M00603\",\"M00600\",\"M00599\",\"M00593\",\\\n",
    "                     \"M00592\",\"M00590\",\"M00585\",\"M00584\",\\\n",
    "                     \"M00581\",\"M00566\",\"M00440\",\"M00438\",\\\n",
    "                     \"M00436\",\"M00423\",\"M00325\",\"M00323\",\\\n",
    "                     \"M00321\",\"M00319\",\"M00302\",\"M00252\",\\\n",
    "                     \"M00251\",\"M00250\",\"M00246\",\"M00245\",\\\n",
    "                     \"M00238\",\"M00234\",\"M00233\",\"M00230\",\\\n",
    "                     \"M00228\",\"M00227\",\"M00225\",\"M00224\",\\\n",
    "                     \"M00223\",\"M00220\",\"M00219\",\"M00217\",\\\n",
    "                     \"M00206\",\"M00210\",\"M00209\",\"M00317\",\\\n",
    "                     \"M00203\",\"M00198\",\"M00200\"],\n",
    " 'nitrogen-sulfur-fatty_acid-photosynthesis':[\"M00082\",\"M00083\",\"M00085\",\\\n",
    "                                              \"M00415\",\"M00086\",\"M00087\",\\\n",
    "                                              \"M00157\",\"M00161\",\"M00162\",\n",
    "                                              \"M00163\"],\n",
    " 'carbon':[],\n",
    " 'phosphotransferase system (PTS)':[],\n",
    " 'oxidative_phosphorylation':[],\n",
    " 'Bacterial Secretion Systems':[\"M00325\",\"M00335\",\"M00336\"], #,\"M00336\" Tat sys\n",
    "    \"Sulfatases\":[]\n",
    "    },\"Add\":{'amino-acids':[],\n",
    " 'CationicAntiomicrobialPeptide_CAMP_resistance':[],\n",
    " 'Vancomycin_Beta-lactamResistance':[],\n",
    " 'Glycosaminoglycan degradation & Synthesis':[],\n",
    " 'N-Glycan biosynthesis':[],\n",
    " 'two-component':[],\n",
    " 'vitamins&cofactors':[\"L-Glutamate=>Uropor-phyrinogen III\",\"L-Threonine=>VitaminB12Coenzyme\",\\\n",
    "                       \"Precorrin 2=>Cob(II)yrinatea,cdiamide via CoPrecorrin\",\\\n",
    "                       \"Precorrin 2=>Cob(II)yrinatea,cdiamide via Precorrin\",\\\n",
    "                       \"Uropor-phyrinogen III=>Precorrin 2\"],\n",
    " 'AminoAcidMetabolism':[\"Pyruvate=>Alanine\",\"Aspartate=>Asparagine\",\"Oxaloacetate=>Aspartate\",\\\n",
    "                            \"Glutamate=>Glutamine\",\"2-Oxoglutarate=>GlutamicAcid\",\\\n",
    "                           \"Threonine=>Glycine\",\"Serine=>Glycine\",\n",
    "                           \"Ornithine=>Arginine\",\"Hydroxy-Pyruvate=>Serine\",\"Glycine=>Serine\"\\\n",
    "                       \"Glutamate=>Proline_v2\",\"Phenylalanine=>Tyrosine\",\"Ornithine=>Proline\"],\n",
    " 'ABC transporters':[],\n",
    " 'nitrogen-sulfur-fatty_acid-photosynthesis':[\"DMSP=>3-(Methylthio)-propanoate(dmdA)\",\n",
    "                                             \"DMSP=>DMS\",\"DMSO=>DMS\",\"DMS=>DMSO\",\"Nitrite=>Nitrate\"],\n",
    " 'carbon':[],\n",
    " 'phosphotransferase system (PTS)':[],\n",
    " 'oxidative_phosphorylation':[],\n",
    " 'Bacterial Secretion Systems':[],\n",
    "  \"Sulfatases\":[\"arylsulfatase B\",\n",
    "    \"N-sulfoglucosamine sulfohydrolase\",\n",
    "    \"N-acetylglucosamine-6-sulfatase\",\n",
    "    \"N-acetylgalactosamine-6-sulfatase\",\n",
    "                \"iduronate 2-sulfatase\"]\n",
    "    }}\n",
    "\n",
    "def add_remove_modules(pathway_modules,operator_dictionary):\n",
    "    for operator, values in operator_dictionary.iteritems():\n",
    "        if operator==\"Add\":\n",
    "            for pathway,items in values.iteritems():\n",
    "                pathway_modules[pathway]=list(set(pathway_modules[pathway]) | set(items))\n",
    "        elif operator==\"Remove\":\n",
    "            for pathway,items in values.iteritems():\n",
    "                pathway_modules[pathway]=list(set(pathway_modules[pathway]) -set(items))\n",
    "            \n",
    "    return pathway_modules\n",
    "\n",
    "MO_pathways=add_remove_modules(MO_pathways,operator_module_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\oxidative_phosphorylation_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\oxidative_phosphorylation_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\oxidative_phosphorylation_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\oxidative_phosphorylation_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Bacterial Secretion Systems_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Bacterial Secretion Systems_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Bacterial Secretion Systems_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Bacterial Secretion Systems_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\N-Glycan biosynthesis_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\N-Glycan biosynthesis_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\N-Glycan biosynthesis_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\N-Glycan biosynthesis_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Glycosaminoglycan degradation & Synthesis_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Glycosaminoglycan degradation & Synthesis_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Glycosaminoglycan degradation & Synthesis_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Glycosaminoglycan degradation & Synthesis_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Vancomycin_Beta-lactamResistance_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Vancomycin_Beta-lactamResistance_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Vancomycin_Beta-lactamResistance_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Vancomycin_Beta-lactamResistance_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\two-component_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\two-component_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\two-component_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\two-component_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\vitamins&cofactors_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\vitamins&cofactors_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\vitamins&cofactors_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\vitamins&cofactors_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\AminoAcidMetabolism_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\AminoAcidMetabolism_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\AminoAcidMetabolism_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\AminoAcidMetabolism_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Sulfatases_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Sulfatases_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Sulfatases_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\Sulfatases_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\ABC transporters_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\ABC transporters_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\ABC transporters_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\ABC transporters_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\carbon_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\carbon_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\carbon_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\carbon_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\nitrogen-sulfur-fatty_acid-photosynthesis_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\nitrogen-sulfur-fatty_acid-photosynthesis_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\nitrogen-sulfur-fatty_acid-photosynthesis_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\nitrogen-sulfur-fatty_acid-photosynthesis_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\phosphotransferase system (PTS)_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\phosphotransferase system (PTS)_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\phosphotransferase system (PTS)_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\phosphotransferase system (PTS)_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\amino-acids_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\amino-acids_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\amino-acids_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\amino-acids_microbes_modules_proportion.png\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\CationicAntiomicrobialPeptide_CAMP_resistance_microbes_modules_proportion.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\CationicAntiomicrobialPeptide_CAMP_resistance_microbes_modules_proportion.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\CationicAntiomicrobialPeptide_CAMP_resistance_microbes_modules_proportion.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\CationicAntiomicrobialPeptide_CAMP_resistance_microbes_modules_proportion.png\n"
     ]
    }
   ],
   "source": [
    "plot_all_old_heatmaps(m, os.path.join(output_dir,\"Plots\"),MO_pathways,uniq_tax_file,\"microbes_modules_proportion\",transpose=True)\n",
    "#plot_all_heatmaps(m, os.path.join(output_dir,\"Plots\"),pathways,uniq_tax_file,\"microbes_modules_proportion\",[0,20,50,60,75,85,100],True)\n",
    "\n",
    "#micro_data=make_microbe_df(m)\n",
    "#plot_all_heatmaps(micro_data, os.path.join(output_dir,\"Plots\"),pathways,tax_file,\"grouped_microbes_modules_proportion\",True)\n",
    "\n",
    "#plot_euk_enrichment_data()\n",
    "\n",
    "#plot_dimensional_scalings()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enriched_tax_data_ramciotti=\"dummy\"\n",
    "enriched_tax_data_plutea=\"\"\n",
    "unenriched_tax_file=os.path.join(*[core,g_drive,\"Honours_Final_report\",\"Relevant_data\",\"926_1392_SeaquenceAllCorals_DNAExtractionTest_16Samplicons_forheatmap.txt\"])\n",
    "unenriched_tax_data=pd.read_csv(unenriched_tax_file, sep=\"\\t\",index_col=[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def non_zero_row(row):\n",
    "    return any(row>0)\n",
    "\n",
    "def prepare_unenriched_otu_hits(unenriched_tax_data,threshold):\n",
    "    #First, remove all rows with zero hits\n",
    "    unenriched_tax_data=unenriched_tax_data.ix[unenriched_tax_data.apply(non_zero_row,axis=1),:]\n",
    "    #print unenriched_tax_data.index.get_level_values(\"Top Blast Hit\")\n",
    "    simplified_tax=unenriched_tax_data.index.get_level_values(\"Top Blast Hit\").str.replace(\"[gsfocpdk]__(?:;|$|\\n)\",'')\\\n",
    "    .str.replace(\" \",\"\").str.replace(\";$\",\"\").str.replace(\"k__.*?;\",\"\").str.replace(\";(.*);\",\";\")\n",
    "\n",
    "    unenriched_tax_data=unenriched_tax_data.reset_index()\n",
    "    #print len(simplified_tax)\n",
    "    #print len(unenriched_tax_data)\n",
    "    #print unenriched_tax_data[\"Top Blast Hit\"]\n",
    "    unenriched_tax_data[\"Top Blast Hit\"]=simplified_tax\n",
    "    \n",
    "    #For duplicate ids add an integer to make it unique\n",
    "    make_uniq_id(unenriched_tax_data,\"Top Blast Hit\")\n",
    "    del unenriched_tax_data[\"OTU\"]\n",
    "    unenriched_tax_data=unenriched_tax_data.set_index(keys=[\"Top Blast Hit\"])\n",
    "    #Normalise the table\n",
    "    unenriched_tax_data=unenriched_tax_data.divide(unenriched_tax_data.sum(axis=0)/100, axis='columns')\n",
    "    unenriched_tax_data=unenriched_tax_data.round(decimals=2)\n",
    "    unenriched_tax_data=remove_less_than(unenriched_tax_data,threshold)\n",
    "    \n",
    "    existing_titles=unenriched_tax_data.columns#[u'PL.Pre.Lysis.1', u'PL.Pre.Lysis.2', u'PL.Pre.Lysis.3']\n",
    "    \n",
    "    new_titles={existing_title:\"P. lutea-{0}\".format(existing_title.split(\".\")[-1]) for existing_title in existing_titles}\n",
    "    \n",
    "    unenriched_tax_data=unenriched_tax_data.rename(columns=new_titles)\n",
    "    unenriched_tax_data=unenriched_tax_data.sort_index(axis=1)\n",
    "    unenriched_tax_data=unenriched_tax_data.sort_index(axis=0)\n",
    "    \n",
    "    return unenriched_tax_data\n",
    "\n",
    "def remove_less_than(unenriched_tax_data,threshold):\n",
    "    def any_row_more(row):\n",
    "        return any(row>=threshold)\n",
    "    \n",
    "    def all_row_more(row):\n",
    "        return all(row>=threshold)\n",
    "    \n",
    "    unenriched_tax_data=unenriched_tax_data.ix[unenriched_tax_data.apply(any_row_more,axis=1),:]\n",
    "    \n",
    "    return unenriched_tax_data\n",
    "\n",
    "def make_uniq_id(unenriched_tax_data,column):\n",
    "    seen_taxa=set([])\n",
    "    #col_name=\"Top Blast Hit\"\n",
    "    taxa_count=defaultdict(int)\n",
    "    for index,row in unenriched_tax_data.copy().iterrows():\n",
    "        taxa=row[column]\n",
    "        if taxa not in seen_taxa:\n",
    "            taxa_count[taxa]+=1\n",
    "            seen_taxa.add(taxa)\n",
    "            taxa=taxa+\"_{0}\".format(taxa_count[taxa])\n",
    "            unenriched_tax_data.set_value(index, column,taxa)\n",
    "        else:\n",
    "            taxa_count[taxa]+=1\n",
    "            taxa=taxa+\"_{0}\".format(taxa_count[taxa])\n",
    "            unenriched_tax_data.set_value(index, column,taxa)\n",
    "    return\n",
    "\n",
    "def prepare_enriched_otus():\n",
    "    return\n",
    "\n",
    "def prepare_enriched_otu_hits(unenriched_tax_data,threshold,contaminants):\n",
    "    #First, remove all rows with zero hits\n",
    "    unenriched_tax_data=unenriched_tax_data.ix[unenriched_tax_data.apply(non_zero_row,axis=1),:]\n",
    "    \n",
    "    ID,taxa_col=unenriched_tax_data.index.names\n",
    "    #print unenriched_tax_data.index.get_level_values(\"Top Blast Hit\")\n",
    "    simplified_tax=unenriched_tax_data.index.get_level_values(taxa_col).str.replace(\"[gsfocpdk]__(?:;|$|\\n)\",'')\\\n",
    "    .str.replace(\" \",\"\").str.replace(\"Root;\",\"\").str.replace(\"(d__Bacteria;|d__Archaea;)\",\"\").str.replace(\";$\",\"\").str.replace(\"k__.*?;\",\"\").str.replace(\";(.*);\",\";\")\n",
    "\n",
    "    unenriched_tax_data=unenriched_tax_data.reset_index()\n",
    "    #print len(simplified_tax)\n",
    "    #print len(unenriched_tax_data)\n",
    "    #print unenriched_tax_data[\"Top Blast Hit\"]\n",
    "    unenriched_tax_data[taxa_col]=simplified_tax\n",
    "    \n",
    "    #For duplicate ids add an integer to make it unique\n",
    "    make_uniq_id(unenriched_tax_data,taxa_col)\n",
    "    del unenriched_tax_data[ID]\n",
    "    unenriched_tax_data=unenriched_tax_data.set_index(keys=[taxa_col])\n",
    "    \n",
    "    #Remove contaminants\n",
    "    unenriched_tax_data=unenriched_tax_data.iloc[~unenriched_tax_data.index.str.contains(\"|\".join(contaminants))]\n",
    "    #Normalise the table\n",
    "    unenriched_tax_data=unenriched_tax_data.divide(unenriched_tax_data.sum(axis=0)/100, axis='columns')\n",
    "    #print enenriched_tax_data.sum(axis=0)\n",
    "    unenriched_tax_data=unenriched_tax_data.round(decimals=2)\n",
    "    unenriched_tax_data=remove_less_than(unenriched_tax_data,threshold)\n",
    "    \n",
    "    existing_titles=unenriched_tax_data.columns#[u'PL.Pre.Lysis.1', u'PL.Pre.Lysis.2', u'PL.Pre.Lysis.3']\n",
    "    \n",
    "    #new_titles={existing_title:\"P. lutea-{0}\".format(existing_title.split(\".\")[-1]) for existing_title in existing_titles}\n",
    "    unenriched_tax_data=unenriched_tax_data.sort_index(axis=1)\n",
    "    unenriched_tax_data=unenriched_tax_data.sort_index(axis=0)\n",
    "    #unenriched_tax_data=unenriched_tax_data.rename(columns=new_titles)\n",
    "    return unenriched_tax_data\n",
    "\n",
    "def prep_n_merge_enriched_otu_hits(list_of_enriched_data,threshold,contaminants):\n",
    "    for i,unenriched_tax_data in enumerate(list_of_enriched_data):\n",
    "        unenriched_tax_data=unenriched_tax_data.ix[unenriched_tax_data.apply(non_zero_row,axis=1),:]\n",
    "\n",
    "        ID,taxa_col=unenriched_tax_data.index.names\n",
    "        #print unenriched_tax_data.index.get_level_values(\"Top Blast Hit\")\n",
    "        simplified_tax=unenriched_tax_data.index.get_level_values(taxa_col).str.replace(\"[gsfocpdk]__(?:;|$|\\n)\",'')\\\n",
    "        .str.replace(\" \",\"\").str.replace(\"Root;\",\"\").str.replace(\"(d__Bacteria;|d__Archaea;)\",\"\").str.replace(\";$\",\"\").str.replace(\"k__.*?;\",\"\").str.replace(\";(.*);\",\";\")\n",
    "\n",
    "        unenriched_tax_data=unenriched_tax_data.reset_index()\n",
    "        #print len(simplified_tax)\n",
    "        #print len(unenriched_tax_data)\n",
    "        #print unenriched_tax_data[\"Top Blast Hit\"]\n",
    "        unenriched_tax_data[taxa_col]=simplified_tax\n",
    "\n",
    "        #For duplicate ids add an integer to make it unique\n",
    "        make_uniq_id(unenriched_tax_data,taxa_col)\n",
    "        del unenriched_tax_data[ID]\n",
    "        unenriched_tax_data=unenriched_tax_data.set_index(keys=[taxa_col])\n",
    "\n",
    "        #Remove contaminants\n",
    "        unenriched_tax_data=unenriched_tax_data.iloc[~unenriched_tax_data.index.str.contains(\"|\".join(contaminants))]\n",
    "        list_of_enriched_data[i]=unenriched_tax_data\n",
    "        \n",
    "    unenriched_single_data=pd.DataFrame([])\n",
    "    for single_df in list_of_enriched_data:\n",
    "        unenriched_single_data=unenriched_single_data.join(single_df,how=\"outer\").fillna(0)\n",
    "    unenriched_tax_data=unenriched_single_data\n",
    "        #Normalise the table\n",
    "    unenriched_tax_data=unenriched_tax_data.divide(unenriched_tax_data.sum(axis=0)/100, axis='columns')\n",
    "    unenriched_tax_data=unenriched_tax_data.round(decimals=2)\n",
    "    unenriched_tax_data=remove_less_than(unenriched_tax_data,threshold)\n",
    "    \n",
    "    existing_titles=unenriched_tax_data.columns#[u'PL.Pre.Lysis.1', u'PL.Pre.Lysis.2', u'PL.Pre.Lysis.3']\n",
    "    unenriched_tax_data=unenriched_tax_data.sort_index(axis=1)\n",
    "    unenriched_tax_data=unenriched_tax_data.sort_index(axis=0)\n",
    "    #new_titles={existing_title:\"P. lutea-{0}\".format(existing_title.split(\".\")[-1]) for existing_title in existing_titles}\n",
    "    \n",
    "    #unenriched_tax_data=unenriched_tax_data.rename(columns=new_titles)\n",
    "    return unenriched_tax_data\n",
    "\n",
    "def clean_column_names_remove_crap(merged_df,key_file,removal_pattern,threshold):\n",
    "    coral_key=pd.read_csv(key_file,sep=\"\\t\")\n",
    "    column_mapping={}\n",
    "    for index,row in coral_key.iterrows():\n",
    "        column_mapping[merged_df.\\\n",
    "                        columns[merged_df.columns.\\\n",
    "                                str.contains(row[\"Sample_id\"])].values[0]]=row[\"Sample_name\"]\n",
    "    merged_df=merged_df.rename(columns=column_mapping)\n",
    "    merged_df=merged_df.iloc[:,~merged_df.columns.str.contains(removal_pattern)]\n",
    "    merged_df=remove_less_than(merged_df,threshold)\n",
    "    return merged_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:147: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "ramciotti_data_file=os.path.join(*[core,g_drive,\"Honours_Final_report\",\"Relevant_data\",\"all_ramciotti_combined_count_table.txt\"])\n",
    "ramciotti_data=pd.read_csv(ramciotti_data_file,sep=\"\\t\",index_col=[0,-1])\n",
    "\n",
    "plutea_data_file=os.path.join(*[core,g_drive,\"Honours_Final_report\",\"Relevant_data\",\"13206to13217_graftm_combined_otu_table.csv\"])\n",
    "plutea_data=pd.read_csv(plutea_data_file,sep=\"\\t\",index_col=[0,-1])\n",
    "\n",
    "unenriched_tax_file=os.path.join(*[core,g_drive,\"Honours_Final_report\",\"Relevant_data\",\"926_1392_SeaquenceAllCorals_DNAExtractionTest_16Samplicons_forheatmap.txt\"])\n",
    "unenriched_tax_data=pd.read_csv(unenriched_tax_file, sep=\"\\t\",index_col=[0,-1])\n",
    "\n",
    "cleaned_ramciotti_data=prepare_enriched_otu_hits(ramciotti_data,1,[\"o__Rickettsiales\",\"k__Eukarya\",\"p__Chlorobi\",\"p__Tenericutes\"])\n",
    "cleaned_plutea_data=prepare_enriched_otu_hits(plutea_data,1,[\"o__Rickettsiales\",\"k__Eukarya\",\"p__Chlorobi\",\"p__Tenericutes\"])\n",
    "\n",
    "unenriched_tax_data=prepare_unenriched_otu_hits(unenriched_tax_data,1)\n",
    "#print unenriched_tax_data.shape\n",
    "#print unenriched_tax_data.index\n",
    "#print set(cleaned_plutea_data.index) & set(cleaned_ramciotti_data.index)\n",
    "#print results.index\n",
    "results=prep_n_merge_enriched_otu_hits([ramciotti_data,plutea_data],1,[\"o__Rickettsiales\",\"k__Eukarya\",\"p__Chlorobi\",\"p__Tenericutes\"])\n",
    "#print results.loc['p__Chloroflexi;c__Anaerolineae_1',:]\n",
    "\n",
    "#cleaned_plutea_data.merge(,how=\"outer\")\n",
    "coral_key_file=os.path.join(*[core,g_drive,\"Honours_Final_report\",\"Relevant_data\",\"coral_key.txt\"])\n",
    "removal_pattern=\"(Porites|Gate)\"\n",
    "merged_metagenome_data=clean_column_names_remove_crap(results,coral_key_file,removal_pattern,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 19)\n"
     ]
    }
   ],
   "source": [
    "#print merged_metagenome_data.shape\n",
    "\n",
    "#merged_metagenome_data=remove_less_than(merged_metagenome_data,1)\n",
    " \n",
    "print merged_metagenome_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_metagenome_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-b9d427796cb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtaxa_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"graftm_enriched\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplot_discrete_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_metagenome_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mticks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplots_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtaxa_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfname_core\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_metagenome_data' is not defined"
     ]
    }
   ],
   "source": [
    "fname_core=\"taxonomic_heatmap\"\n",
    "taxa_type=\"graftm_enriched\"\n",
    "ticks=[0,0.5,1,2,3,4,5,10,15,20,25]\n",
    "plot_discrete_heatmap(merged_metagenome_data,ticks,\"\",plots_dir,taxa_type,fname_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "fname_core=\"taxonomic_heatmap\"\n",
    "taxa_type=\"16S_unenriched\"\n",
    "ticks=[0,0.5,1,2,3,4,5,10,15,20,25]\n",
    "plot_discrete_heatmap(unenriched_tax_data,ticks,\"\",plots_dir,taxa_type,fname_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Abundance parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:49: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:51: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "contig_coverage_file=os.path.join(*[core,g_drive,\"parse_coverage\",\"bin_contig_sep_coverages.tsv\"])\n",
    "\n",
    "total_read_counts=os.path.join(*[core,g_drive,\"parse_coverage\",\"final_bin_read_counts.tsv\"])\n",
    "\n",
    "key_file=os.path.join(*[core,g_drive,\"Honours_Final_report\",\"contig_file_type.txt\"])\n",
    "\n",
    "def load_contig_values(contig_cov_file):\n",
    "    df=pd.read_csv(contig_cov_file,index_col=[0,1],sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def load_total_counts_file(tot_read_counts):\n",
    "    df=pd.read_csv(tot_read_counts,index_col=[0],sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def load_type_key(key_file):\n",
    "    df=pd.read_csv(key_file,sep=\"\\t\",index_col=[0])\n",
    "    return df\n",
    "\n",
    "def calculate_coverage(ctg_file):\n",
    "    '''Calculates the length normalised average coverage for each bin the average.'''\n",
    "    length_column=ctg_file[[ctg_file.columns[0]]*11]\n",
    "    ctg_file[ctg_file.columns[1:]]=ctg_file[ctg_file.columns[1:]]*length_column.values\n",
    "    total_file=ctg_file.groupby(level=[0]).sum()\n",
    "    coverage_values=total_file\n",
    "    coverage_values=coverage_values[coverage_values.columns[1:]]/coverage_values[[\"Length\"]*11].values\n",
    "    coverage_values=coverage_values.div(coverage_values.sum(axis=0),axis=1)\n",
    "    coverage_values.index=coverage_values.index.str.extract(\"(U_[0-9]{5})\")\n",
    "    coverage_values=coverage_values.rename(index=uniq_genome_taxonomy)\n",
    "    coverage_values.columns=coverage_values.columns.str.extract(\"([^/]+\\.bam)\")\n",
    "    coverage_values=coverage_values.rename(columns=type_key[\"Sample.1\"])\n",
    "    coverage_values=coverage_values.groupby(level=0,axis=1).mean()\n",
    "    return coverage_values\n",
    "\n",
    "\n",
    "ctg_file=load_contig_values(contig_coverage_file)\n",
    "\n",
    "cts_file=load_total_counts_file(total_read_counts)\n",
    "\n",
    "type_key=load_type_key(key_file)\n",
    "    \n",
    "merged_info=pd.DataFrame.join(cts_file,type_key)\n",
    "\n",
    "length_column=ctg_file[[ctg_file.columns[0]]*11]\n",
    "ctg_file[ctg_file.columns[1:]]=ctg_file[ctg_file.columns[1:]]*length_column.values\n",
    "total_file=ctg_file.groupby(level=[0]).sum()\n",
    "coverage_values=total_file\n",
    "coverage_values=coverage_values[coverage_values.columns[1:]]/coverage_values[[\"Length\"]*11].values\n",
    "coverage_values=coverage_values.div(coverage_values.sum(axis=0),axis=1)\n",
    "coverage_values.index=coverage_values.index.str.extract(\"(U_[0-9]{5})\")\n",
    "coverage_values=coverage_values.rename(index=uniq_genome_taxonomy)\n",
    "coverage_values.columns=coverage_values.columns.str.extract(\"([^/]+\\.bam)\")\n",
    "coverage_values=coverage_values.rename(columns=type_key[\"Sample.1\"])\n",
    "coverage_values=coverage_values.groupby(level=0,axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P.lutea_1</th>\n",
       "      <th>P.lutea_2</th>\n",
       "      <th>P.lutea_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rhodospirillales_1</th>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.010584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poribacteria_1</th>\n",
       "      <td>0.015171</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.020469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syntrophobacteraceae_1</th>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.017976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemm-2_1</th>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.010033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAUC26f_1</th>\n",
       "      <td>0.012849</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.013751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Endozoicomonadaceae_1</th>\n",
       "      <td>0.009785</td>\n",
       "      <td>0.816135</td>\n",
       "      <td>0.006757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acidobacteria_1</th>\n",
       "      <td>0.014266</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.021222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemm-2_2</th>\n",
       "      <td>0.016072</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.023950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poribacteria_2</th>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.023129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhodothermaceae_3</th>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.009902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nitrosopumilus_2</th>\n",
       "      <td>0.005707</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.038162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iii1-15_2</th>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.030532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloroflexi_2</th>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.010568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloroflexi_3</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.012204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhodothermaceae_1</th>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.007953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloroflexi_1</th>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.009760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhodothermaceae_2</th>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.008238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTCC2089_1</th>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.020768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iii1-15_1</th>\n",
       "      <td>0.013027</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.027720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mle1-48_1</th>\n",
       "      <td>0.015892</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.015142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caldilineaceae_1</th>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.010390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nitrosopumilus_1</th>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.024022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acidimicrobiales_1</th>\n",
       "      <td>0.189373</td>\n",
       "      <td>0.009592</td>\n",
       "      <td>0.002726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poribacteria_3</th>\n",
       "      <td>0.137247</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poribacteria_4</th>\n",
       "      <td>0.052155</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>0.051752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAUC34f_1</th>\n",
       "      <td>0.015879</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syntrophobacteraceae_2</th>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>0.009653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sva0725_1</th>\n",
       "      <td>0.016635</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.027107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anck6_1</th>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.020714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poribacteria_5</th>\n",
       "      <td>0.028258</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.078029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemm-4_1</th>\n",
       "      <td>0.012039</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.017704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemm-2_3</th>\n",
       "      <td>0.017867</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>0.041032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poribacteria_6</th>\n",
       "      <td>0.017635</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.052578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Piscirickettsiaceae_1</th>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.010206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actinobacteria_1</th>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.012908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poribacteria_7</th>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.018328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTCC2089_2</th>\n",
       "      <td>0.008195</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.019631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC214_1</th>\n",
       "      <td>0.040416</td>\n",
       "      <td>0.047434</td>\n",
       "      <td>0.050828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAR202_1</th>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.014476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4b_1</th>\n",
       "      <td>0.004439</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAUC26f_2</th>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.018608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TK18_1</th>\n",
       "      <td>0.004460</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.018051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chromatiales_1</th>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.014360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAUC34f_2</th>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.011433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAR202_2</th>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.016609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloroflexi_4</th>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.011492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloroflexi_5</th>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.013361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhodanobacter_1</th>\n",
       "      <td>0.138057</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nitrospiraceae_1</th>\n",
       "      <td>0.017008</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.042987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloroflexi_6</th>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.015394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4b_2</th>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.008062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAR202_3</th>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.017433</td>\n",
       "      <td>0.009135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        P.lutea_1  P.lutea_2  P.lutea_3\n",
       "bin                                                    \n",
       "Rhodospirillales_1       0.003800   0.001294   0.010584\n",
       "Poribacteria_1           0.015171   0.001941   0.020469\n",
       "Syntrophobacteraceae_1   0.012778   0.001142   0.017976\n",
       "Gemm-2_1                 0.009072   0.000993   0.010033\n",
       "PAUC26f_1                0.012849   0.001167   0.013751\n",
       "Endozoicomonadaceae_1    0.009785   0.816135   0.006757\n",
       "Acidobacteria_1          0.014266   0.001046   0.021222\n",
       "Gemm-2_2                 0.016072   0.001473   0.023950\n",
       "Poribacteria_2           0.010642   0.003923   0.023129\n",
       "Rhodothermaceae_3        0.003330   0.000924   0.009902\n",
       "nitrosopumilus_2         0.005707   0.003001   0.038162\n",
       "iii1-15_2                0.008690   0.000917   0.030532\n",
       "Chloroflexi_2            0.005420   0.001194   0.010568\n",
       "Chloroflexi_3            0.003397   0.001020   0.012204\n",
       "Rhodothermaceae_1        0.002052   0.001089   0.007953\n",
       "Chloroflexi_1            0.009059   0.000802   0.009760\n",
       "Rhodothermaceae_2        0.002053   0.001081   0.008238\n",
       "HTCC2089_1               0.004744   0.001998   0.020768\n",
       "iii1-15_1                0.013027   0.001461   0.027720\n",
       "mle1-48_1                0.015892   0.001237   0.015142\n",
       "Caldilineaceae_1         0.004222   0.002340   0.010390\n",
       "nitrosopumilus_1         0.014652   0.005513   0.024022\n",
       "Acidimicrobiales_1       0.189373   0.009592   0.002726\n",
       "Poribacteria_3           0.137247   0.004194   0.001185\n",
       "Poribacteria_4           0.052155   0.009490   0.051752\n",
       "PAUC34f_1                0.015879   0.005048   0.011653\n",
       "Syntrophobacteraceae_2   0.011730   0.008193   0.009653\n",
       "Sva0725_1                0.016635   0.001216   0.027107\n",
       "Anck6_1                  0.012677   0.000951   0.020714\n",
       "Poribacteria_5           0.028258   0.004916   0.078029\n",
       "Gemm-4_1                 0.012039   0.001356   0.017704\n",
       "Gemm-2_3                 0.017867   0.008650   0.041032\n",
       "Poribacteria_6           0.017635   0.004638   0.052578\n",
       "Piscirickettsiaceae_1    0.007776   0.001123   0.010206\n",
       "Actinobacteria_1         0.007236   0.000903   0.012908\n",
       "Poribacteria_7           0.009804   0.004966   0.018328\n",
       "HTCC2089_2               0.008195   0.001384   0.019631\n",
       "EC214_1                  0.040416   0.047434   0.050828\n",
       "SAR202_1                 0.006713   0.001293   0.014476\n",
       "A4b_1                    0.004439   0.001186   0.005898\n",
       "PAUC26f_2                0.004912   0.000752   0.018608\n",
       "TK18_1                   0.004460   0.000860   0.018051\n",
       "Chromatiales_1           0.006080   0.001535   0.014360\n",
       "PAUC34f_2                0.003695   0.001296   0.011433\n",
       "SAR202_2                 0.004267   0.001549   0.016609\n",
       "Chloroflexi_4            0.004139   0.000807   0.011492\n",
       "Chloroflexi_5            0.003862   0.000897   0.013361\n",
       "Rhodanobacter_1          0.138057   0.001763   0.000866\n",
       "Nitrospiraceae_1         0.017008   0.002546   0.042987\n",
       "Chloroflexi_6            0.005302   0.001291   0.015394\n",
       "A4b_2                    0.004544   0.001049   0.008062\n",
       "SAR202_3                 0.010921   0.017433   0.009135"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file C:\\Users\\Alex\\Google Drive\\Honours\\metabolic_analysis\\Plots\\genome_bins_mapped_taxonomic_heatmap.eps\n",
      "Saving the file C:\\Users\\Alex\\Google Drive\\Honours\\metabolic_analysis\\Plots\\genome_bins_mapped_taxonomic_heatmap.svg\n",
      "Saving the file C:\\Users\\Alex\\Google Drive\\Honours\\metabolic_analysis\\Plots\\genome_bins_mapped_taxonomic_heatmap.pdf\n",
      "Saving the file C:\\Users\\Alex\\Google Drive\\Honours\\metabolic_analysis\\Plots\\genome_bins_mapped_taxonomic_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "fname_core=\"taxonomic_heatmap\"\n",
    "taxa_type=\"genome_bins_mapped\"\n",
    "ticks=[0,0.5,1,2,3,4,5,10,15,20,25]\n",
    "plot_discrete_heatmap(coverage_values*100,ticks,\"\",plots_dir,taxa_type,fname_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.eps\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.svg\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.pdf\n",
      "Saving the file C:\\Users\\Baker\\Google Drive\\Honours\\metabolic_analysis\\Plots\\16S_unenriched_taxonomic_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "fname_core=\"taxonomic_heatmap\"\n",
    "taxa_type=\"16S_unenriched\"\n",
    "ticks=[0,0.5,1,2,3,4,5,10,15,20,25]\n",
    "plot_discrete_heatmap(unenriched_tax_data,ticks,\"\",plots_dir,taxa_type,fname_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
